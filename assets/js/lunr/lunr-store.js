var store=[{title:"The Story of gREST, A Graph-based Restful API Development Framework",excerpt:"More than a year ago, I was looking for a framework to build my services with which can support graph-based databases, namely Neo4j, and give me the freedom to customize it however I wanted. As my main language of choice was (and is) Python, I tried to look for frameworks written in Python. Many options came to mind, including Django, Flask, Tornado and many others. One of the biggest challenges I have faced was that none of them supported graph-based databases natively, meaning that they help the developer quickly get up and running with minimum amount of code. At that time I had to use one of the available Object-Graph Mappers (OGM) and would have to write my own set of functions to handle CRUD on database models. Such a simple thing for many other SQL and some NoSQL databases would lead to thousands of lines of code to handle creation, deletion and modification of nodes and their relationships.   After struggling to find such a framework, I decided to write my own set of functions as views to handle different endpoints I needed to get the job done.   As I have used and worked with Flask in other projects, I decided to go the path of using a micro-framework and develop what I needed and use available extensions. To quickly get up and running I used Flask-Classy which is an extension of the Flask framework that supports class-based views for each endpoint supporting various HTTP verbs. It has been deprecated and I have replaced it with Flask-Classful, which is a fork of the same project, but is actively developed.   My OGM of choice was (and still is) Neomodel, a very-well documented, feature-rich OGM for the Neo4j database. Neomodel had all the abstraction I wanted to quickly develop a set of models for nodes and their relations.   Mixing Flask, Flask-Classy, Neomodel allowed me to write a big class containing all the methods to expose views of my database models. I named the class generic_view, so that I can subclass it for each view and use it to serve each endpoint.   Everything was fine until I thought that I should pay back to the FOSS community. So I decided to extract that piece of code from the project and make it a separate project that can be used as a simple extension to the Flask framework.   So, I created gREST, a restful API development framework on top of Python, Flask, Neo4j and Neomodel. Its primary purpose is to ease development of restful APIs with little effort and minimum amount of code.   I used to make it a python package and distribute it on PYPI. I wrote unit-tests, a simple README and two simple examples to help developers know more about the workings of gREST.   I also used pytest for my unit-tests, Travis-CI for continuous integration and running tests, Coveralls for code coverage and Gitter for a place to answer developers\u2019 questions. As I was concerned about the licensing of the project, I have released it under GPLv3 license and used the FOSSA license check services to know if there are licensing mismatches. The FOSSA team deserves a medal for its service.   It\u2019s more than a year from the time I created my initial commit of the project and as per analytics, it has been downloaded more than 36,000 times and growing.                     Download count of the gREST framework according to the Google BigQuery results on the the-psf:pypi dataset            As I have twitted about it when I\u2019ve made the project public, it has also been featured on the Neo4j\u2019s blog.   Last but not least, I tried my best to fix bugs and make improvements to the initial idea of gREST, but it needs the help of the community to be further developed. I also hope that it continues to serve the community well. My next article would be a tutorial on making a simple restful API using gREST.   Update: At the time of writing this article, there was  only 36k downloads, while now, June 24 2020, it\u2019s almost 120k, according to PePy, which, I think, is impressive.  ",categories:["Python","gREST","Neo4j","Flask","RESTful API"],tags:["Python","gREST","Neo4j","Flask","RESTful API"],url:"https://mostafa.dev/blog/the-story-of-grest-a-graph-based-restful-api-development-framework",teaser:null},{title:"The Story of Farhang, A Bilingual Dictionary Production System",excerpt:"More than 9 years ago, when I was studying towards a B.A. degree in German Language Translation Studies, I used to know and be a student of a great professor, Prof. Saied Firuzabadi, at my university, Azad University of Tehran, which is a great author, translator and professor with a big track record of publishing many books and articles.   On the summer of that year, he used to have an extracurricular class for teaching pragmatic translation skills. I just enrolled, since I knew I would learn something great.   On the first day, he told us about his new project and achievement, in which he authored a German-Persian dictionary worth of more than 10,000 A4 papers of handwritten words and meanings.   At that time, his biggest struggle with the work was that he couldn\u2019t find great typists to enter that information into a more manageable, printable format that can be published on paper. He used to tell us about the frustrations he had at that time.   The day after that day, I used to think about ways that this amount of information can be managed. Then I found some pieces of software that can handle that amount.   One of those software was a great proprietary software, IDM DPS. The other was the TshwaneLex from TshwaneDJe Software. The former was used by very big dictionary production companies and was a very expensive piece of software. The latter was affordable, but didn\u2019t have great support for right-to-left languages (like Persian) at that time.   It seemed like I should create a custom solution to handle this data entry, typesetting, generating publishing-ready PDFs and so on. What I did was to do minor research on ways to manage these information and transform them into usable formats. This was the start of the story.   The other day I went to the class and told the professor that I have a solution to your data-management problem. He was astounded to hear what I said. I went to the white-board and gave the class a very basic data-entry application sketch that was connected to a SQL database to store and retrieve the information that can later be used. Almost everyone was astounded.   There were two obstacles along the way. The first one was that all that information should be entered into the application manually and the second was the application itself. It must have been developed in a forward-looking way, so that the data could be later used, so we needed to make the data as structured as possible. For the first obstacle, a group of five students volunteered to do the job. The second obstacle should have been handled by me alone.   Later that month, I sketched out a set of UIs for the data-entry application and with the help of great mono- and bi-lingual dictionaries in other languages and by analyzing those dictionaries, I was able to discern the way they separate, label and manage that vast amount of information that at first looked like an unmanageable haystack.   As an avid advocate and user of FOSS software, I used Python, Qt4 and PostgreSQL to design UI and database tables alike. The very first MVP of the application was great enough for data-entry by the team. I was able to cross-compile the application for them since they all were Windows users.   The team managed to do the data-entry using the application. It took around 6 months to finish the \u201cdata-entry\u201d job. At the same time, I was trying to devise a solution for generating a PDF that looked like a bilingual dictionary with all the details \u2014 CMYK, good quality tables and figures, margins, \u2026 \u2014 needed for publishing companies to able to consume the PDF and prepare, print and produce the hard-cover version.   I tried different solutions, from generating PDFs using various libraries to producing files that can be read by applications like Adobe InDesign. Almost none of those solutions worked because of the same RTL language problem and the support of them inside those libraries and applications.   Then I came across LaTeX and XeTeX. By creating a somewhat simple class file in TeX, I was able to produce bilingual dictionaries with qualities of those very well-known dictionaries like Cambridge, Oxford, Longman, Duden and Langenscheidt.                     TeX File Used to Produce PDF Output                              First Page of the Dictionary in PDF (Letter A)            The dictionary was typeset with the first version of the Farhang DPS, which was developed on Linux. Later when editing the dictionary, I faced many problems with the SQL approach, then I migrated the data into a document-oriented NoSQL database, MongoDB. This choice allowed me to have a better locality of information inside a document with nested information inside the document itself, thus avoiding SQL JOINS and their constraints.   Also I changed the development environment to use Windows and the Visual Studio. I used C# and MongoDB to develop the second version of Farhang. I used Mercurial (hg) to do version control on my code which I later migrated the repository into Git, so that I can make it FOSS and publish it on GitHub.                     Second Version of Farhang DPS            In 2018, the hard-cover version was published by Langenscheidt Verlag in Germany. It is also planned to be published as a Mobile App and hard-cover version in my own country, too. The dictionary is around 2700 pages.   There were days of sweat and tears to review and edit the data and generate new files which were then used by different applications to produce output PDF files, and it was a very painstaking and arduous set of tasks to make all this possible for the language learners and translators to use this treasure to find their ways through the texts.  ",categories:["Farhang","Dictionary","Dictionary Writing","Persian","German"],tags:["Farhang","Dictionary","Dictionary Writing","Persian","German"],url:"https://mostafa.dev/blog/the-story-of-farhang-a-bilingual-dictionary-production-system",teaser:null},{title:"Secure Code Review and Penetration Testing of Node.js and JavaScript Apps",
excerpt:"Security is an illusion and being secure is a relative matter. This means you should always have an eye on your security from any perspective: Physical, human, social, corporate and IT security. Since any system, given enough resources \u2014 knowledge, tools and time, can be hacked.   My goal of writing this to show you how to approach and do a systematic secure code review and penetration testing for securing your Node.js and JavaScript apps which are very widespread nowadays. This being abundant enables attackers to have enormous amount of systems that are written in this language and that run on Node.js, browser and the like.   There are two different approaches on this: a) Secure Code Review as a defensive approach to find flaws in a system and trying to secure it. b) Penetration Testing as an offensive approach to find vulnerabilities and weaknesses on a live system.   A. Secure Code Review      Security code review is the process of auditing the source code for an application to verify that the proper security controls are present, that they work as intended, and that they have been invoked in all the right places. Code review is a way of ensuring that the application has been developed so as to be \u201cself-defending\u201d in its given environment. \u2014 OWASP Code Review Introduction    As you see, it\u2019s all about auditing, verification and analysis. The OWASP community has proposed a good starting point to do this job. It is called \u201cThe OWASP Code Review Project\u201d. It is an technical guide to help the code reviewers find the most flaws under a unified framework.   Two versions of this technical guide is published and version 2.0 being the most recent one, which was published 14 July 2017. The second version, which is thoroughly updated, consists of an introduction to the subject matter, a methodology and a set of technical references for the \u201cOWASP Top 10\u201d things to look for while reviewing code. Various diagrams, methods, risk models and techniques has been discussed. It is argued that the code review process should be a integrated into the Software Development Life-cycle (SDLC), from pre-commit to post-commit phases. Then a risk-based approach to code review has been proposed which tries to anlayze risk with different techniques. A set of code review preparation steps follows a set of static analysis tools. The last part show the S-SDLC as an \u201cApplication Threat Modeling\u201d approach which consists of three steps:      Step 1: Decompose the application.  Step 2: Determine and rank threats.  Step 3: Determine countermeasures and mitigation.    Some tools for threat modeling has also been discussed. Lastly, a set of metrics are shown that can help the reviewer with quality and security characteristics of the code-base. Thereafter the code crawling practice is briefly introduced.   The technical reference section consists of the top 10 topics to consider while reviewing code:      A1-Injection  A2-Broken Authentication and Session Management  A3-Cross-Site Scripting (XSS)  A4-Insecure Direct Object Reference  A5-Security Misconfiguration  A6-Sensitive Data Exposure  A7-Missing Functional Level Access Control  A8-Cross-Site Request Forgery (CSRF)  A9-Using Components with Known Vulnerabilities  A10-Unvalidated Redirects and Forwards    This list is partly on par with what is known as the \u201cOWASP Top 10 Project\u201d, with some minor differences, which I do recommend both of these guides to anyone who wants to review code systematically, find flaws and eventually secure the system.   B. Penetration Testing   The second part, after reviewing code, is the penetration testing, which also consists of a set of steps to find and pinpoint vulnerabilities and weaknesses from an attacker\u2019s point of view. Basically, you hack to secure!      A penetration test, colloquially known as a pen test, is an authorized simulated cyber-attack on a computer system, performed to evaluate the security of the system. \u2014 Wikipedia, Penetration test    Under the \u201cOWASP Top 10 Project\u201d, three different versions of a set of guidelines has been published up until now, with the most recent one being the 2017 version.   This guide consists of top 10 vulnerabilities to look for while testing an application/system:      A1:2017-Injection  A2:2017-Broken Authentication  A3:2017-Sensitive Data Exposure  A4:2017-XML External Entities (XXE)  A5:2017-Broken Access Control  A6:2017-Security Misconfiguration  A7:2017-Cross-Site Scripting (XSS)  A8:2017-Insecure Deserialization  A9:2017-Using Components with Known Vulnerabilities  A10:2017-Insufficient Logging &amp; MonitoringAll these security risks may not be present all at once in one application, but one may lead to others and possibly to the compromise of the whole system or network.    There are a set of cheat sheets from the OWASP community that you can read to get an overview of what to expect while securing or testing a system:      https://github.com/OWASP/CheatSheetSeries/tree/master/cheatsheets                                                                               OWASP/CheatSheetSeries                   The OWASP Cheat Sheet Series was created to provide a concise collection of high value information on specific application security topics. - OWASP/CheatSheetSeries                        https://github.com/           Recommended Books      https://www.oreilly.com/library/view/defensive-security-handbook/9781491960370                                                                               Defensive Security Handbook                   Despite the increase of high-profile hacks, record-breaking data leaks, and ransomware attacks, many organizations don\u2019t have the budget to establish or outsource an information security (InfoSec) program, forcing them to \u2026  - Selection from Defensive Security Handbook [Book]                        https://www.oreilly.com/              https://www.oreilly.com/library/view/securing-node-applications/9781491982426/                                                                               Securing Node Applications                   Security incidents are indeed on the rise, but according to one authoritative analysis, 85% of all successful exploits focus on the top ten security vulnerabilities. In this report, author Chetan \u2026  - Selection from Securing Node Applications [Book]                        https://www.oreilly.com/              https://github.com/getify/You-Dont-Know-JS                                                                               getify/You-Dont-Know-JS                   A book series on JavaScript. @YDKJS on twitter. Contribute to getify/You-Dont-Know-JS development by creating an account on GitHub.                        https://github.com/              https://exploringjs.com/                                    Exploring JS: JavaScript books for programmers                        Axel has been writing about the future of JavaScript since early 2011.      ...                        https://exploringjs.com/           C. Where to Look for Vulnerability Listings   There are many databases that list vulnerabilities and their information and relations. For example, if it is an open-source project, you need to visit the \u201cIssues\u201d section, either on GitHub, GitLab or elsewhere.   There are many bug trackers out there. Some of which are focused on security, e.g. https://security-tracker.debian.org/tracker/.   If it is a proprietary project, you usually should look for public databases, like the company\u2019s website or these websites:      https://www.exploit-db.com/                                    Offensive Security\u2019s Exploit Database Archive                                           https://www.exploit-db.com/              https://cve.mitre.org/                                    CVE -  Common Vulnerabilities and Exposures (CVE)                    \xa0Common Vulnerabilities and Exposures...                        https://cve.mitre.org/              https://www.cvedetails.com/                                    CVE security vulnerability database. Security vulnerabilities, exploits, references and more                    \t\t\t\t\twww.cvedetails.com \t\t\t\t provides an easy to use web interface to CVE vulnerability data. \t\t\t\tYou can browse for vendors, products and versions and view cve entries, vulnerabil...                        https://www.cvedetails.com/              https://nvd.nist.gov/                                     \tNVD - Home                    This is a potential security issue, you are being redirected to https://nvd.nist.gov...                        https://nvd.nist.gov/           D. Programming Knowledge and Experience   It should never be underestimated that for great code review, one needs great programming skills, or at least, an understanding of what is used on the project. Since JavaScript ecosystem has been evolved over the years and many libraries, engines and practices has been developed, one can easily get lost in this vast amount of information.   I do recommend you to have a look at these two gold mines:      https://github.com/sorrycc/awesome-javascript                                                                               sorrycc/awesome-javascript                   \ud83d\udc22 A collection of awesome browser-side  JavaScript libraries, resources and shiny things. - sorrycc/awesome-javascript                        https://github.com/              https://github.com/sindresorhus/awesome-nodejs                                                                               sindresorhus/awesome-nodejs                   :zap: Delightful Node.js packages and resources. Contribute to sindresorhus/awesome-nodejs development by creating an account on GitHub.                        https://github.com/           E. Tools of the Trade   Now that we have a solid ground in terms of methodology and knowledge to work on, we should look at the available tools that can help an reviewer/attacker do their job.   I just list these tools, which I have tested my projects with, and it is up to you to read the documentation and figure out how they work. They are very easy to use in my view.   I have stumbled upon https://github.com/mre/awesome-static-analysis, https://github.com/jesusprubio/awesome-nodejs-pentest and https://www.owasp.org/index.php/Source_Code_Analysis_Tools as an starting point for doing the static analysis of the code. Many tools are listed, some old and some new. What I used for my project is as follows:   Counting lines of code in your project      https://www.npmjs.com/package/cloc                                                                               cloc                   An npm module for distributing cloc by Al Danial https://github.com/AlDanial/cloc                        https://www.npmjs.com/           Auditing your code to see if there are any known vulnerabilities      https://snyk.io/                                                                               Snyk | Develop Fast. Stay Secure                   Snyk helps you use open source and stay secure. Continuously find and fix vulnerabilities for npm, Maven, NuGet, RubyGems, PyPI and much more.                        https://snyk.io/              https://github.com/OSSIndex/auditjs                                                                               sonatype-nexus-community/auditjs                   Audits an NPM package.json file to identify known vulnerabilities. - sonatype-nexus-community/auditjs                        https://github.com/              https://github.com/ajinabraham/NodeJsScan                                                                               ajinabraham/nodejsscan                   nodejsscan is a static security code scanner for Node.js applications. - ajinabraham/nodejsscan                        https://github.com/              https://github.com/RetireJS/retire.js                                                                               RetireJS/retire.js                   scanner detecting the use of JavaScript libraries with known vulnerabilities - RetireJS/retire.js                        https://github.com/              https://codeclimate.com/                                                                               Engineering Metrics to Improve Continuous Delivery Practices | Velocity                   Code Climate provides automated code review for your apps, letting you fix quality and security issues before they hit production. We check every commit, branch and pull request for changes in quality and potential vulnerabilities. If an issue is found, you're notified immediately - it's that simple.                        https://codeclimate.com/           Linting and static analysis of your code inside your editor      https://github.com/dustinspecker/awesome-eslint                                                                               dustinspecker/awesome-eslint                   A list of awesome ESLint plugins, configs, etc. Contribute to dustinspecker/awesome-eslint development by creating an account on GitHub.                        https://github.com/              https://github.com/eslint/eslint                                                                               eslint/eslint                   Find and fix problems in your JavaScript code. Contribute to eslint/eslint development by creating an account on GitHub.                        https://github.com/              https://github.com/nickdeis/eslint-plugin-no-secrets                                                                               nickdeis/eslint-plugin-no-secrets                   An eslint plugin to find strings that might be secrets/credentials - nickdeis/eslint-plugin-no-secrets                        https://github.com/              https://github.com/nodesecurity/eslint-plugin-security                                                                               nodesecurity/eslint-plugin-security                   ESLint rules for Node Security. Contribute to nodesecurity/eslint-plugin-security development by creating an account on GitHub.                        https://github.com/              https://github.com/Rantanen/eslint-plugin-xss                                                                               Rantanen/eslint-plugin-xss                   ESLint plugin for XSS detection. Contribute to Rantanen/eslint-plugin-xss development by creating an account on GitHub.                        https://github.com/              https://github.com/SonarSource/eslint-plugin-sonarjs                                                                               SonarSource/eslint-plugin-sonarjs                   SonarJS rules for ESLint. Contribute to SonarSource/eslint-plugin-sonarjs development by creating an account on GitHub.                        https://github.com/              https://github.com/mozfreddyb/eslint-config-scanjs                                                                               mozfreddyb/eslint-config-scanjs                   umbrella config to achieve scanjs-like functionality through eslint - mozfreddyb/eslint-config-scanjs                        https://github.com/           Useful JavaScript libraries for secure development      https://www.defensivejs.com/                                    Defensive JavaScript home                     DJS is a defensive subset of JavaScript: code in this  subset runs independently of the rest of the JavaScript environment.  When propertly wrapped, DJS code can run safely on unt...                        https://www.defensivejs.com/           Cloud services that offer security as a service      https://www.cloudflare.com/                                                                               Cloudflare - The Web Performance &amp; Security Company   | Cloudflare                    Here at Cloudflare, we make the Internet work the way it should. Offering CDN, DNS, DDoS protection and security, find out how we can help your site.                         https://www.cloudflare.com/              https://www.sqreen.com/                                                                               Application Security Management Platform | Sqreen                   Learn more about Sqreen's application security platform that helps teams protect applications, increase visibility and secure code.                        https://www.sqreen.com/              https://detectify.com/                                                                               Leading website vulnerability scanner | Free 14 day trial                   Web security issues are a major pain, thankfully our website vulnerability scanner identifies issues before they become a problem. Find vulnerabilities before hackers do!                        https://detectify.com/              https://semmle.com/                                                                               Semmle - Code Analysis Platform for Securing Software                   Semmle's code analysis platform helps teams find zero-days and automate variant analysis. Secure your code with continuous security analysis and automated code review.                        https://semmle.com/           Most of what I recommended up until now was about secure code review. The following tools are useful for penetration testing:      http://sqlmap.org/                                    sqlmap: automatic SQL injection and database takeover tool                   sqlmap is an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers. It comes with a pow...                        http://sqlmap.org/              https://portswigger.net/burp                                                                               Burp Suite - Application Security Testing Software                   Get Burp Suite. The class-leading vulnerability scanning, penetration testing, and web app security platform. Try for free today.                        https://portswigger.net/              https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project                                    OWASP ZAP Zed Attack Proxy | OWASP                   OWASP Chapters All Day - 24hrs of Virtual Chapter Meetings Learn More...                        https://owasp.org/              https://github.com/loadimpact/k6                                                                               loadimpact/k6                   A modern load testing tool, using Go and JavaScript - https://k6.io - loadimpact/k6                        https://github.com/              https://nmap.org/                                    Nmap: the Network Mapper - Free Security Scanner                   Nmap (\"Network Mapper\") is a free and open source (license) utility for network discovery and security auditing.  Many systems and network administrators also find it useful for tas...                        https://nmap.org/              https://www.getpostman.com/                                                                               Postman | The Collaboration Platform for API Development                   Simplify each step of building an API and streamline collaboration so you can create better APIs\u2014faster                        https://www.postman.com/           Guidelines and best practices      https://github.com/Checkmarx/JS-SCP                                                                               Checkmarx/JS-SCP                   JavaScript Secure Coding Practices guide. Contribute to Checkmarx/JS-SCP development by creating an account on GitHub.                        https://github.com/              https://owasp.org/www-project-secure-coding-practices-quick-reference-guide/migrated_content                                    OWASP Secure Coding Practices-Quick Reference Guide                   OWASP Chapters All Day - 24hrs of Virtual Chapter Meetings Learn More...                        https://owasp.org/              https://www.veracode.com/security/javascript-security                                                                               JavaScript Security                   What Is JavaScript?                        https://www.veracode.com/              https://security.berkeley.edu/secure-coding-practice-guidelines                                    Secure Coding Practice Guidelines | Information Security Office                   UC Berkeley security policy mandates compliance with\xa0Minimum Security Standard for Electronic Information\xa0for devices handling covered data. \xa0The recommendations below are provided ...                        https://security.berkeley.edu/              https://www.javascriptjanuary.com/blog/defensive-javascript                                                                               Defensive JavaScript  \u2014 JavaScript January                   Many thanks to Google\u2019s      Mike Samuel      for contributing this article on JavaScript security.                        https://www.javascriptjanuary.com/              https://snyk.io/blog/comparing-react-and-angular-secure-coding-practices/                                                                               Comparing React and Angular secure coding practices | Snyk                   Welcome to Snyk's State of JavaScript frameworks security report 2019, this section of the report is about Angular and React projects overall security posture.                        https://snyk.io/              https://github.com/lirantal/awesome-nodejs-security                                                                               lirantal/awesome-nodejs-security                   Awesome Node.js Security resources. Contribute to lirantal/awesome-nodejs-security development by creating an account on GitHub.                        https://github.com/           Now is your time to sharpen your swords \u2694\ufe0f and test them to be ready for attack. Be sure not to cut your fingers. \ud83d\ude09   Although I have presented ways to review code and penetration test on JavaScript and Node.js, the same principles and practices apply to other languages, too. You just have to look up the information on that language.   Finally, your comments and suggestions are much appreciated.  ",categories:["JavaScript","Nodejs","Security","Pentest","Code Review"],tags:["JavaScript","Nodejs","Security","Pentest","Code Review"],url:"https://mostafa.dev/blog/secure-code-review-and-penetration-testing-of-node-js-and-javascript-apps",teaser:null},{title:"Beginner\u2019s Guide to Load Testing with k6 \u2014 Part 1",excerpt:"Update: This article is part of a series on load testing. I will try to regularly publish related topics in this series. I won\u2019t repeat this table of contents on every single article, so please check this very first one to get new updates. Also I will try to link articles together, so that you can easily follow the next one in the series. Ever since, these has been published:      Part 1 \u2014 The Prelude     Part 2 \u2014 Performance Goals and k6 Metrics     Part 3 \u2014 How to Write &amp; Run a Load Test Using k6    Let\u2019s start with some questions:      Have you ever thought about ways to find out if your infrastructure setup could stand under high loads of users?   Have you ever been able to run a load test without dealing with so many configurations and steep learning curve of current testing tools?   Have you ever thought about ways to script your way through testing, either for load/performance or otherwise?   Have you ever thought that performance testing can be simplified and be included as part of your CI/CD process? You are not alone in that. I remember days when I was the lead developer of a team working on developing an API to serve our ever-increasing set of Apps with many micro-services like content- and user-management, billing, analytics and the like. At that time, we used to use the latest best practices in software engineering in source control and CI/CD processes. We have set up our CI/CD process using docker and hosted GitLab and used GitLab CI to deal with CI process. After a while of testing and code-push, we have decided to go with an alpha-stage API that can answer our needs with a new App we used to develop.   After a while of development, testing and improvement, we decided to test our API under load to see if it can hold up to the promises. Since we have seen that after 200,000 users subscribing and 60,000 of them using one of our other Apps, there is an slow-down on the request/response flow to the other API, which roughly had the same setup.   So, in order to not miss the opportunity of (load) testing before customers coming in, we decided to investigate ways to load test our API. After our investigations into Locust, Apache JMeter and the like, we decided to go with Apache JMeter, which was (and still is) the tool of the trade. We thought it would be a good candidate for this purpose. It took us almost three days to be able to learn and execute simple load tests with the GUI version. We were able to produce load test from one server and analyze the results. The results were good, because it gave us a bit more insight into what we should expect from our API, in terms of number of concurrent users and our API\u2019s response time under load.   But later we didn\u2019t use the tool, since there was no clear way to integrate it into our CI/CD process at that time, and we didn\u2019t have enough time to learn and use it. We wanted something more developer-friendly, more integrable to our workflow, and easier to learn. Nowadays, it has been improved very much and it integrates with CI/CD pipelines easily.   Now, I am working for a company, Load Impact, which deals with load and performance testing as their core business. They have a FOSS, easily scriptable tool (JavaScript), called k6, which can be used for load testing. They also offer a cloud service on top of the same exact tool, for provisioning and running the tests, from different regions of the world Cloud Execution, and it finally give you a nice dashboard with analyzed results of the tests with shiny charts. It also has good integrations with CI/CD tools and platforms.   Load test script written in JavaScript:   import http from 'k6/http'; import { check, sleep } from 'k6';  export let options = {   stages: [     { duration: '30s', target: 20 },     { duration: '1m30s', target: 10 },     { duration: '20s', target: 0 },   ], };  export default function() {   let res = http.get('https://httpbin.org/');   check(res, { 'status was 200': r =&gt; r.status == 200 });   sleep(1); }                     k6 Cloud Dashboard            Enough said! Let\u2019s dive deep into what load testing is and how we can use k6 for doing load testing on our infrastructure. I will try to write more about the cloud service in the future, too.   What is Load/Performance Testing?   Performance testing is a subset of performance engineering in computer science practice which deals with quality assurance of software by putting pressure and workload, either manually or automatically, on the software to see how it behaves and to make sure it is responsive and stable enough under that load. It\u2019s all about expectations. For example, you expect your API or website to serve 10K users simultaneously, but when you put pressure on it, it quickly becomes evident that your calculations differs from the reality and the reverse proves to be the case. Then you should analyze the weak points in your design or setup to see how you can benefit from high load.   There are many types of tests catered for different purposes in performance testing (Source: https://en.wikipedia.org/wiki/Software_performance_testing):      Load testing: basically, putting load on the system to see how it behaves.   Stress testing: load testing to find the maximum amount of load the system can handle.   Soak testing: load testing a system continuously and monitoring for memory leaks and behavior of the system.   Spike testing: load testing with sudden increase or decrease of the load.   Breakpoint testing: like stress testing, but incremental load is put on the system over time to see how it behaves.   Configuration testing: changing configuration to see how the system behaves under different configurations, under load.   Isolation testing: Isolating a fault domain and repeating the test to confirm the failure.   Internet testing: global load testing for big companies to see how to system behaves from different regions. The most common purpose of performance testing is to find out how reliable, stable, performant and responsive the system is. Metrics like throughput and response time is a good measure of such a system.   k6: Scripting and Running Load Tests and Interpreting The Results   k6 is a Free/Open-Source tool, written in Go, which can ingest tests written in JavaScript (ES5.1+) and turn them into requests to load test your website or API.   The easiest possible way to use k6 is to install it on your machine using a official prebuilt binary version. k6 supports Microsoft Windows with an MSI installer, GNU/Linux with an APT repository and Apple macOS using brew.   There is also an official docker image. It is not recommended for beginners to build from source, but you always have the option to do that.   You can then run your installed k6 command with the following script:   import { check } from \"k6\"; import http from \"k6/http\";  export default function() {   let res = http.get(\"https://test.loadimpact.com/\");   check(res, {     \"is status 200\": (r) =&gt; r.status === 200   }); };   A sample test script for k6   Once you have saved this script somewhere accessible to k6, naming it script.js or your own desired file name, you can run it with 10 virtual users (VU) and over a period of 30 seconds, as follows:   k6 run -u 10 -d 30s script.js   It basically runs k6 on your machine using 10 virtual users over the course of 30 seconds, and it checks to see if the test URL returns 200 (OK) in all tests.   The output of running this command would be as below. I recommend you to have a look at it to better understand different metrics, checks and other key-values. This output will be thoroughly explained in the next articles.   By interpreting the current output, you can see that the script is locally executed for the duration of 30 seconds and spawned 10 separate virtual users (vus) to test the URL. You can also see the highlighted green checked item, \u201cis status 200\u201d, that signifies that our check has passed for 100% of the cases, meaning all cases.   The total data sent and received in 2775 requests is also present. For each HTTP requests, there is a http_req_ metric (key) with 5 different values, each corresponding to the average, minimum, median, maximum, 90th percentile and 95th percentile of all the requests.                     Terminal output of the sample test script for k6            This was a very short introduction to performance/load testing and the k6 tool. I will try to write more advanced load testing articles to show you how to load test your website and/or API.   This series will continue with more in-depth performance/load testing articles assuming the readers don\u2019t have previous experience on this topic.   In the next article in this series, I will try to talk about performance goals and k6 metrics, which are the building blocks of load testing.   As always, I do really appreciate your suggestions, comments and inputs on this. And I would like to thank Pepe Cano and Robin Gustafsson for their valuable comments and suggestions on this article and the series.  ",categories:["Performance Testing","Load Testing","Load Impact","k6","Guides And Tutorials"],tags:["Performance Testing","Load Testing","Load Impact","k6","Guides And Tutorials"],url:"https://mostafa.dev/blog/beginner-s-guide-to-load-testing-with-k6-part-1",teaser:null},{title:"Beginner\u2019s Guide to Load Testing with k6 \u2014 Part 2",
excerpt:'Performance testing is an umbrella term for a group of tests that encompasses many types of tests, as discussed in the first part of this series. Each test type tries to answer a set of questions to make the performance testing process more goal-oriented. This means that just running tests is not enough, you have to have a set of goals to reach.   Since we\u2019re testing an API or a website, the following goals may be relevant, in which you can choose either one or more:      Concurrency: systems which would set this goal, usually have a concept of end-user and need to see how the system behaves while many concurrent users try to access the system. They basically want to test how many of requests fail/pass under high loads of users. This both includes many concurrent users and each requesting multiple resources at the same time.   Throughput: systems with no concept of end-users, would set this goal to see how the system behaves overall, while there is a ton of requests/responses coming in/out of the system.   Server response time: this goal signifies the time it takes from the initial request from the client to the server up until a response is sent back from the server.   Regression testing: sometimes the goal is not to put \u201cheavy load\u201d on the system, but is more about \u201cnormal load\u201d and functional and regression testing to see how a change would affect our system\u2019s performance and if it still adheres to our defined SLAs. The general idea is to measure how a system or system of systems behave(s) under heavy load, in terms of speed, scalability, stability and resiliency. Each of which can be measured by these goals.      Speed can be measured by time it takes for request to be handled by the server and how much time it takes for this request/response to happen.   Scalability can be measured by how well the system scales if the load is increased and by measuring if it sustains over a period of time under this load.   Stability can be measured by how well the system sustains the load and to see if it stands against a high number of errors and events and still stays responsive and stable.   Resiliency can be measured by how the system recovers from crashes and down-times and responds to requests, after putting too much or too frequent load on it and eventually crashing the system.   Rerunning Tests to Verify the Results   You can rerun the tests to see if they hold almost the same results during different tests and compare the tests to see if they deviate.   If they are almost the same, you can analyze the tests and derive your results, otherwise you should pinpoint where it deviates and try to find a way to prevent it from happening, like a bottleneck.   k6 and the Metrics   k6 supports a set of built-in and custom metrics that can be used to measure various things and to either achieve the above mentioned goals or prove them wrong. The metrics that can be used to define custom metrics are: Counter, Gauge, Rate and Trend.                     k6 built-in metrics            As you\u2019ve probably seen above, these following tables describes reported built-in metrics, present on all tests:                  metric       type       description                       vus       Gauge       Current number of active virtual users                 vus_max       Gauge       Max possible number of virtual users (VU resources are preallocated to ensure performance will not be affected when scaling up the load level)                 iterations       Counter       The aggregate number of times the VUs in the test have executed the JS script (the default function). Or in case the test is not using a JS script but accessing a single URL the number of times the VUs have requested that URL                 data_received       Counter       The amount of received data                 data_sent       Counter       The amount of data sent                 checks       Rate       Number of failed checks.           Credits: k6 built-in metrics                  metric       type       description       datatype                       http_reqs       Counter       How many HTTP requests has k6 generated in total       integer                 http_req_blocked       Trend       Time spent blocked (waiting for a free TCP connection slot) before initiating request       float                 http_req_looking_up       Trend       Time spent looking up remote host name in DNS       float                 http_req_connecting       Trend       Time spent establishing TCP connection to remote host       float                 http_req_tls_handshaking       Trend       Time spent handshaking TLS session with remote host       float                 http_req_sending       Trend       Time spent sending data to remote host       float                 http_req_waiting       Trend       Time spent waiting for response from remote host (a.k.a. time to first byte or TTFB)       float                 http_req_receiving       Trend       Time spent receiving response data from remote host       float                 http_req_duration       Trend       Total time for the request. It\u2019s equal to http_req_sending + http_req_waiting + http_req_receiving (i.e. how long did the remote server take to process the request and respond (without the initial DNS lookup/connection times)       float           Credits: k6 HTTP-specific built-in metrics   Custom (non-built-in) Metrics   1. Counter   This is a simple cumulative counter that can be used to measure any cumulative value like number of errors during the test.   import { Counter } from "k6/metrics"; import http from "k6/http";  var myErrorCounter = new Counter("my_error_counter");  export default function() {   let res = http.get("https://test.loadimpact.com/404");   if(res.status === 404) {     myErrorCounter.add(1)   } };   k6 Counter metric   As you can see in the above example, it counts the number of 404 errors that are returned by the test. The result is evident in the screenshot below:                     Results of k6 Counter metric            Since it is a beginner\u2019s guide, I try to stick with simple examples, but you can extend and customize them to your specific case.   2. Gauge   This metric lets you keep the last thing that is added to it. It\u2019s a simple over-writable metric that holds its last added value.   This metric can be used to retain the last value of any test item, be it response time, delay or any other user-defined value.   If you run the following code, you\u2019ll see that it catches the latest error code, which is 404.   import { Gauge } from "k6/metrics"; import http from "k6/http";  var myGauge = new Gauge("my_gauge");  export default function() {   let res = http.get("https://test.loadimpact.com/404");   myGauge.add(res.status); };   k6 Gauge metric   The result of the test is presented in the screenshot below.                     Results of k6 Gauge metric            3. Rate   This built-in metric keeps the rate between non-zero and zero/false values. For example if you add two false and one true value, the percentage becomes 33%.   It can be used to keep track of the rate of successful request/responses and compare them with errors.   In the following piece of code, you can see that I added res.error_code as a measure to see how many errors I\u2019ll catch.   import { Rate } from "k6/metrics"; import http from "k6/http";  var myRate = new Rate("my_rate");  export default function() {   let res = http.get("https://test.loadimpact.com/404");   myRate.add(res.error_code); };   k6 Rate metric   Below is the result of the test, which is 100% errors.                     Results of k6 Rate metric            4. Trend   This metric allows you to statistically calculate your custom value. It will give you minimum, maximum, average and percentiles, as is evident in the above screenshots for http_req* requests.   import { Trend } from "k6/metrics"; import http from "k6/http";  var myTrend = new Trend("my_trend");  export default function() {   let res = http.get("https://test.loadimpact.com/");   myTrend.add(res.timings.sending + res.timings.receiving); };   k6 Trend metric   The above example of trend metric shows how to calculate the sending and receiving time without taking into account the waiting time. The result is shown below:                     Results of k6 Trend metric            In this part, I\u2019ve tried to describe the goals of performance testing and how one can use metrics to achieve those goals. In the next sections, I\u2019ll try to go more in-depth and present you more details on how to define custom metrics and how to use them.   Now that you have a good grasp of performance goals and k6 metrics, you can move to the next article, in which I try to show you how to write and run a k6 script.  ',categories:["Performance Testing","Load Testing","Load Impact","k6","Guides And Tutorials"],tags:["Performance Testing","Load Testing","Load Impact","k6","Guides And Tutorials"],url:"https://mostafa.dev/blog/beginner-s-guide-to-load-testing-with-k6-part-2",teaser:null},{title:"Beginner\u2019s Guide to Load Testing with k6 \u2014 Part 3",excerpt:'Now that you know the basics and have defined your performance goals, it is now time to write your load-test script and run it using k6. Since k6 scripts can be written in JavaScript, you can easily write them if you have a minimum knowledge of this programming language.   k6 Test Life-cycle   There are three parts in each test script (actually there is more), which is depicted in the below screenshot:                     Different parts of the k6 script               Imports: This is obviously where you can import k6 Script API, and other JavaScript modules (libraries) that you desire to make use of. These modules can be loaded using various methods: bundled NPM modules on your local machine (browserified, but browser APIs are not supported) and remote modules (from a URL, even CDNJS and GitHub). Hint: Make sure to make the libraries available to the dockerized version of k6 via mounted volumes, otherwise your script does not find the imports.   Init code: This is the part of the script that is outside the exported \u201cdefault\u201d function. It is usually used to provide options to the whole test, how to run the test, how to distribute it on the cloud, etc. Obviously, it is used for initialization of the whole test itself.   VU code: k6 supports a feature called virtual users. This means that you can use separate \u201csmart\u201d virtual users to test your system. The code in this section which is inside the exported \u201cdefault\u201d function, is ran over and over inside each VU and the aggregated results of all these VUs are processed and reported by k6. This is where you define your test scenario, which is thoroughly explained further in this article. There are other stages in the life-cycle of a k6 test, which includes setup and tear-down, which, clearly, are separate from the init and VU code.   How to Run k6?   There are various methods of running k6. Since the k6 is distributed in source code and binary, you can easily grab and install a binary version on your machine and run it. Supported operating systems include GNU/Linux, Microsoft Windows and Apple macOS. There is also a dockerized version that you can run on your docker setup.   No matter where you run the k6, it provides you with the same experience. Here\u2019s the help of the k6 command:                     k6 help            The format of running k6 is like \u201ck6 [command]\u201d, which you can then pass your desired command and let it run by k6. Basic available commands, as of version 0.25.1, is as follows. Plus you can get help for each command with this combination:   k6 &lt;command&gt; --help      help: shows the above help, well, obviously.   pause: pause a running test.   resume: resume a paused test.   run: run a test with various flags, e.g.\u2013*paused *to run a script in paused mode.   stats: shows statistics about the currently running or paused test entailing number of VUs.   status: show the status of current k6 instance, either running, paused, tainted and the number of VUs.   version: guess what?   These are more advanced commands that I will try to cover in the coming articles:      archive: creates a bundled tar file of your script along with all the dependencies, which you can later use with \u201ck6 run\u201d.   login: authenticates with k6 Cloud service and provides an authentication token to be used by k6.   cloud: runs your authenticated test on the k6 cloud service.   convert: browsers can log requests/responses as HAR (HTTP Archive) files, which you can then convert the HAR file using k6. k6 creates a script from the HAR file, which you can edit and run locally or on the cloud.   inspect: basically outputs the consolidated script options for that script or .tar bundle.   scale: scale a paused/running test with a new number of virtual users (VUs). It can do this either for local or for cloud execution. Each of these commands has a rich set of options/flags to pass in order to control the behavior of k6. I strongly recommend you to have a look at them, since sometimes you find gems inside.   Test Scenario &amp; Interpretation of Results   Test scenario is an inseparable part of each test. If you want to test your API or website, you have to have a scenario and eventually turn it into a script, and then pass it to k6 to run.   Let\u2019s have an example:      A user checks of the API is up, hence checking the heart beat of our API and then tries to send further messages.   Then they try to create a token to be able to access other parts of our API or the private areas.   Finally they try to access an endpoint of our API.   [heart-beat]-&gt;[generate-token]-&gt;[get-list-of-users]   You also want to add some more criteria and thresholds, to be able to discern more meaningful results from the k6. Although some information, like threshold is new in this article, I\u2019ll try to explain them completely in the upcoming article. You want this scenario to be tested by 100 individual users, starting with 10 users, ramping up to 100 and the gradually down to 0, and to keep the response time of all request below 500ms.   The script would be something like this:   import { check, group, sleep } from \'k6\'; import http from \'k6/http\';  export let options = {   max_vus: 100,   vus: 100,   stages: [     { duration: "30s", target: 10 },     { duration: "4m", target: 100 },     { duration: "30s", target: 0 }   ],   thresholds: {     "RTT": ["avg&lt;500"]   } }  export default function() {   group(\'v1 API testing\', function() {     group(\'heart-beat\', function() {       let res = http.get("https://httpbin.org/get");       check(res, { "status is 200": (r) =&gt; r.status === 200 });     });      group(\'login\', function() {       let res = http.get("https://httpbin.org/bearer", {         headers: { "Authorization": "Bearer da39a3ee5e6b4b0d3255bfef95601890afd80709" }       });       check(res, {         "status is 200": (r) =&gt; r.status === 200,         "is authenticated": (r) =&gt; r.json()["authenticated"] === true       });     });      group(\'access an endpoint\', function() {       let res = http.get("https://httpbin.org/base64/azYgaXMgYXdlc29tZSE=");       check(res, {         "status is 200": (r) =&gt; r.status === 200,         "k6 is awesome!": (r) =&gt; r.body === "k6 is awesome!"       });     });   });   sleep(1); }   I have used httpbin.org as a kitchen-sink for testing k6 with this script. The results of running test for five minutes is as follows:                     Output of k6 scenario script            The output start with the logo of the k6 being printed to the screen. After that comes the method of execution, being \u201clocal\u201d in my case. The output is redirected to the terminal and the name of the script is \u201ctest.js\u201d.   Duration and iterations are empty, since they are automatically calculated from the stages. The number of (max) VUs are 100 and the progress bar shows that it is done in five minutes.   \u201cv1 API Testing\u201d and other group names are followed by a white full block sign \u201c\u2588\u201d and the checks are all ticked and green, indicating they are passed in all cases.   All the checks in the tests are passed, specifically 56213 checks in 33741 requests. The amount of data sent and received, vus, iterations and some other metrics are also shown.   The very important part is the metrics that start with http. They signify the average, minimum, maximum, p(90) and p(95) of the amount of time each request or group of request has taken to complete.   Since we have defined the average threshold to be 500ms. Since the average time of all requests hasn\u2019t taken that long, the test is passed.   The results shown on screen is the aggregated results on all the tests. If you want to be able to use all the generated data, and not only the aggregated ones, you should use the -o flag to send the output to either a files, a software or the cloud service.   The raw results can be written to a JSON file using JSON plugin. There are other plugins that push the metrics to InfluxDB, Apache Kafka, StatsD or Datadog. The last (and probably the best) option would be to use the Load Impact plugin that streams your test results to the Load Impact cloud platform.   In this article I tried my best to show you how to write a pretty simple test scenario script and how to run to the load test using k6.   In the next article I\u2019ll try to turn concepts I\u2019ve talked about on the second article \u2014 Performance Goals and k6 Metrics \u2014 into features supported by k6 that you can use to have a successful load or acceptance testing.   As always, I do welcome your feedback. Please don\u2019t hesitate to write your comments, questions and feedback below.  ',categories:["Performance Testing","Load Testing","Load Impact","k6","Guides And Tutorials"],tags:["Performance Testing","Load Testing","Load Impact","k6","Guides And Tutorials"],url:"https://mostafa.dev/blog/beginner-s-guide-to-load-testing-with-k6-part-3",teaser:null},{title:"Employment Contract",
excerpt:"Disclaimer:      I am not an attorney, a lawyer or an advocate whatsoever, so seek the help of a professional on your case before signing anything.   I tried my best to be as anonymous as possible about the attributions and references in this article and I strongly deny any attributions.   *I try to be as fair and as informative as possible to both parties, and I take no responsibility on any of the views expressed in this article. *USE THEM AT YOUR OWN RISK!   Views expressed in this articles are solely those of the author.   Always try to seek clarification if you don\u2019t understand a clause.   Introduction   There are, at least, two sides (or parties) to each employment contract. The employer who gives you an offer after careful consideration and the employee, that you are, who want to weigh your options and eventually negotiate, sign or reject the contract (or the offer) altogether.   So far, I haven\u2019t seen any firm, sound and perfect employment contract. There are either some minor or major flaws. It seems that no professional have had any chance to review them. It sometimes seems epidemic to me to face contracts, which are flawed in their basic clauses.   Some of them try to use a friendly voice, some are aggressive and some are cryptic. Those who talk friendly are mostly the ones their offer is quickly accepted, since friendly manners can be seen in their legal documents. The aggressive ones are those who try to stop you from practicing your rights and try to be as strict as possible, which they can\u2019t. The cryptics try to hide behind the misunderstood and misinterpreted laws, rules and regulations.   It seems like the employer and the employee are on different pages. In order to show them the light, I\u2019ll try to explain how to approach a contract and how to eventually negotiate, sign or reject it. It\u2019s a live document and I\u2019ll try to update it as much as I learn more about the laws, rules and regulations.   An employment contract, \u201ccontract\u201d from now on, is a set of terms, rules, conditions, laws and regulations which allows the employer and employee to know their rights and responsibilities and try to work together, as set in the contract. The contact is almost always bilateral, meaning both parties have responsibilities and obligations. This bilateral contract lets them seek the practice of law \u2014 and sometimes force in worst cases \u2014 to deal with issues arising from any party not conforming to the contract.   I presume that you have passed all the interviews, tests, on-site, off-site, online and everything else and you\u2019ve been offered a contract, so you are thinking: \u201cnow what?\u201d. What I am trying to do is to answer your \u201cnow what?\u201d question.   A contract usually includes some or most of the following topics, which I will try to explain in greater detail hereafter. It is going to be a long text, which obviously I do recommend you to read, but otherwise just read the interesting parts. The first part is the terms of employments and in the second part I will explain the decision-making process.   Always be aware of the gray areas in the contract, since there are places where it is hard to decide what to do while something happens. Always try to ask for more information and clarification on those clauses, since this clarification would later help you be assured that you can do your job with no interruptions.   Throughout the article I try to give examples of a software developer contract, but the general terms apply to all employment contracts.   \ud83d\uddf9 Terms of Employment   1. Employee Responsibilities or The Assignment   Usually this part includes information about the job responsibilities and how you should do the job. It is either brief like a sentence or two, or divided into multiple statements that try to clarify or include as much as possible. Sentences starting with \u201cyour initial responsibilities, but not limited to, \u2026\u201d are usual statements that, instead of clarification, try to include as much responsibility as possible and also account for future changes to your responsibilities. These types of statement are those you should be fully aware of, since when the description of your job or responsibility changes, you cannot deny it, because you already accepted it. Although this doesn\u2019t limit you to do anything legal to take your rights back.   An example template of these statements would be like this:      We strive to give you the opportunity of grow and blah blah, but we reserve the right to change the terms whenever we see fit. Your initial responsibilities is/will be, but not limited to:         To work in the team \u2026     To do this and that \u2026     To make sure everything is OK \u2026      2. Reporting Structure   Some companies are hierarchical and some are flat, or so they say. In every possible organization, there will be a way to report to a higher authority on the matter. It would be your mentor, manager, leader, CTO, CEO or anyone else who is responsible and in charge for making sure that you are doing good and are happy with your job and task. They are the ones you should trust and try not to hide information from, even if you\u2019ve made a mistake. Telling them that you\u2019ve done something great or made a mistake, is the key to the prosperity of the whole company. Otherwise this debt will amass and would lead to losing the job or even the chance of company being dissolved.   Sometimes you\u2019ll be assigned mentors to let you familiarize with the company, the projects and the team. These mentors are responsible for all of these and they usually are old members or employees of the company and are more senior.   3. Description of The Work   In this section, you will be brief about the description of your work, which usually is different than your responsibilities, which I have discussed above. Your responsibilities are general sentences that give you an idea of what the general vision for this role is, according to company\u2019s vision of personal and corporate grow. This section is more detailed and is usually about the specifics of your job.   Usually these topics are discussed:      Start date: a strict or flexible start date, where you should, obviously, start your work.   Location of work: usually this is the company\u2019s address or if your company has branches overseas or need you to travel, those places are usually mentioned here. Some companies try to make it as general as possible to make room for future branches to be included in your contract.   Hours of work: either strict or flexible timing is what you should take into consideration. This is sometimes negotiable to some extent, but not always. Strict work hours are the usual 9\u20135, and flexibility in timing usually depends on the type of the work.   Type of work: office-based, remote, work from home (WFH) are some possible combinations of types of works you can do to fulfill your responsibility. Office-based is the most obvious one, which you should \u201cgo to work\u201d every single morning and usually come back in the evening. Remote and WFH jobs are those you either work from overseas or work from home. Usually a mix of these is also possible, in which, you come to work on certain days and you have the option to work from home on the others.   Relocation support: If you live far from the company you\u2019ve applied for, they may try to help you move to the nearest location to the company\u2019s office. Either the company\u2019s human resources (HR) team handles the relocation or they seek the help of another relocation services company dedicated to help you with relocation, either with or without your family.   Visa sponsorship: people in certain countries need visas to to be able to travel and work in a foreign country. Some companies offer this and would help you along the process.   Work permit: along with the sponsorship of the visa, a work permit is usually needed to be accompanied by the visa, in order for the employee to be able to work legally for the company in a foreign country.   4. Background Check   Depending on the country and company, there would be a background check on your profile. This is what happens when you are going to work for high-profile companies or companies that are working in the security industry. This is not something secretive and the company must seek your permission to do this. Along with your offer of employment, you will be presented with another form to sign to give consent about it. The form should include everything they try to do to do background check, like the name of the security consultation company, what they look for and how they\u2019ll perform this action. This is not something to worry about, unless you have something to hide. Be aware and prepared,since almost all your claims will be verified. These are usually checked: where you worked, who you worked with, your qualifications and certifications and all other related pieces of information.   5. Internship and Induction   Some companies have internship periods where you will be employed full- or part-time by the company afterwards. This is something that fresh-graduates are usually familiar with, since it is usually the case that they don\u2019t have enough work experience, and they want to gain this experience, so they seek internship at big or small companies, either paid or otherwise.   For more experienced personnel, there will be an induction period, of three to six months, where they will be assessed by their contribution to the project(s), team and company or any other Key-Performance Indicators (KPIs). This is usually not the case for all and sometimes is the formality of the job or contract to include this statement. Usually the notice period for the induction period is much shorter, which I\u2019ll talk about it in further topics.   6. Prohibition   Sometimes the company wants to legally prevent you from doing two or more businesses at the same time. This means that you cannot be employed full-time by two companies at the same time. Although they cannot stop you from doing this, but this has its own consequences. Some companies prevent you from doing any other type of business while you are employed by the company or even afterwards, which I\u2019ll talk about it later. Although this seems unfair in some circumstances, it is there to ensure that you are fully focused on the company\u2019s assignment(s).   You, as an employee, should be fully aware of this. Sometimes it is negotiable. For example, the company may accept the you study and work at the same time or you are working on an open-source project at the same time that usually does not take a lot of your time. So, be prepared to tell them what you do, besides your normal company hours. Since a mutual understanding and trust would later help your achieve great results.   7. Confidentiality   Company\u2019s trade secrets are very important to the prosperity and growth of the company. So you should always be aware and careful not to share any trade secrets with anyone, especially on social media, where the outreach would be devastating to the company\u2019s plans and future. Also, not everything is confidential, unless clearly stated in the contract, otherwise just ask your manager or mentor about it.   Statements would be like this:      All information obtained in the course of work with COMPANY-X shall be deemed to be strictly confidential.  No such information shall be divulged to any third party, or use for any purpose other than on the performance of your duties.    8. Non-Disclosure Agreement, a.k.a. NDA   Almost always, a NDA follows the main contract, which is a type of amendment or appendix to the main contract. NDA is a legal contract between parties in the contract that outlines the confidentiality of information, knowledge or material that is shared between the parties by the means of the contract. It goes on by defining different terms that apply to the sharing of these confidential trade secrets. As noted above, confidential information which are obtained from the company by any means by either parties, belong to the same company, unless the company explicitly confirms that it can be shared with third-parties. For example, you cannot share details of your tech-stack with the world, unless the company confirms it in writing that you can.   9. Non-Competition Clause   Sometimes companies can\u2019t stand competition. So, they try to drag you into signing amendments or appendices called non-competition agreement. Sometimes it is written in main contract as the non-competition clause. This is to ensure that the other party, employee in this case, does not try to engage in any other similar business or trade that competes or is in direct contrast with the current one. Some see this as restrictive, others not. But as long as you try not to do the same exact thing with a competing business, you are good to go, although no one can stop you. I do recommend you to ask for clarification on any of the terms that you seem to be restrictive and limiting.   10. Ownership Agreements   When you start working for an employer, you should realize that you cannot use anything, including intellectual property, source code, documentation or otherwise, which has been created or produced at workplace for any other purpose than the job itself. That holds true, unless you have written permission from the company, that you can use it somewhere else, or make it open-source. This means that the company owns every single thing that is created on work-hours or even off. Sometimes it is hard to discern which is which, since you may create a similar piece of product on your spare time based on the ideas that are those of the company as a whole.   As a general piece of advice, do not try to use the company\u2019s assets and properties for other means than the company\u2019s interests, unless you know what you are doing or you have explicit written permission.   11. Employee Benefits and Perks   Usually each job at each company has its own set of benefits. The benefits range from housing, transportation, profit sharing and health and retirement benefits to entertainment in or outside the company. Each company has its own set of rules for this. The general rule is that at least some of them are provided by each company. These big and small incentives for the employees are \u201crewards\u201d on top of the salary to improve employee retention in the company. Just keep in mind that some of these benefits are taxable in some countries.   12. Leave   During your employment, you are entitled by law to leave your job, while maintaining your status in the company. Each country and each company has their own set of laws and regulations. Generally you can have vacations, paid time off (PTO), holidays and other types of leaves, which you are usually paid. This is also negotiable to some extent.   There is another type of leave, called leave of absence, which is either requested by the employer or by the employee based on various grounds. They are also either paid or unpaid. The difference with the above mentioned leaves is that it is used for special circumstances, like active duty from reserve military personnel. This is not usually mentioned in the contract and is left for the law to decide.   13. Remote Work   As is described above, remote work has its own set of rules and practices. In this section of the contract, rules regarding remote work is outlined. As with every other types of work, it has its own set of potentials and drawbacks. Besides that, all the terms should be laid out clearly enough in order not to confuse either parties.   Generally, remote work is described as a type of work arrangement, in which the employee need not travel or commute to the workplace to do the job. There are two types of remote work. Remote work on specific days of the week (or month), in which the employee is permitted to work from home on certain days handle tasks and get the job done. On other days, they should be at the workplace to have meetings and synchronize with the team. The other type is the true remote work, in which the cost of travel, usually overseas, is unjustifiable or unreasonable for both parties. So the employer uses various types of remote communication tools to facilitate the job.   This topic probably needs its own article to be fully addressed, but as a whole, contracts for true remote work is different in many terms with normal contracts, since the employer usually need not provide extra benefits to the employee.   14. Tax   One of the most important and complicated topics of all contracts is tax. There are various taxes to be aware of. The most important one is the income tax, in which you pay some amount of your salary to your country\u2019s tax office (the government). Usually the employer handles all tax deductions and payments, unless your country\u2019s laws are different. The other type is social security contributions or payroll tax, with each country having its own set of laws and regulations, sometimes paid all by the employer and sometimes shared between the two.   You always should be aware of all the tax regulations of your country which are directly or indirectly related to you and try to seek consultation of a professional on this. Otherwise you\u2019ll get into serious trouble on your tax clearance at the end of fiscal year.   15. Insurance   Insurances is very varied, but are usually in two categories: mandatory and voluntary. Mandatory insurances are those imposed on employers either by law or by unions\u2019 agreements to help the employees in need. Since we are not always healthy and there are mishaps and illnesses. This mandatory type of insurance helps the employees and their family to benefit from public healthcare services. Some of them are for welfare of the society and some others are for pension, i.e. when you get old and retire. Usually all of these mandatory insurances are basic types of insurance. In order to increase the coverage of healthcare services to include private healthcare services and have bigger retirement payments and the like, you should have some form of private insurance. Some companies help with group private insurance(s), but I do recommend you to find your own health and life insurance, alongside that.   16. Salary Review and Bonuses   It usually happens that you are not happy with your job because you are not compensated well. So, this clause ensures that you have the right to negotiate higher amounts while working for the company. Some include it by default and some don\u2019t. So it is up to you to request this to be included in your contract.   Over time and as the company makes profit, there will be bonuses divided between employees. Usually it is up to the company to decide if they want to pay it or not. If they decide to include such a clause in your contract, they try their best not to put any type of pressure on themselves to pay you any type of bonus and they make it look usually like a unilateral promise with no obligations.   17. Conflict of Interest and Dispute Resolution   Life is not all about fairies. Conflict of interest arises when the interest of oneself opposes the interest of others and vice versa. I recommend you to try not to go into those directions, when it is not necessary. If you do, then there are various ways to mitigate such conflicts, but the rule of thumb is not to give away your right to seek help of a professional and eventually seek practice of law, in case the tensions are heightened. There are various ways to resolve disputes and deal with conflict of interest, which is out of scope of this article.   18. Resignation and Notice Period   Sometimes you no longer want to work for the company, either you\u2019ve found a new job or want to conquer new frontiers. In this case, you usually resign. You should give a letter of resignation to your employer, thank them about the opportunity they provided you and eventually address the matter of resignation. While resigning, you can not vanish into thin air afterwards. You should respect (the rules of) the contract in this case, too. Notice period is the amount of time you should give to your employer to be aware of your leave and to try to help you with off-boarding and other paperwork. Usually the duration of the notice period before and after induction period is different.   19. Termination   Sometimes it doesn\u2019t work well between you and the employer and they want to terminate the contract. In this case you are either dismissed or laid off, which depends on the decision of the employer. Other times, the duration of contract is ended and they don\u2019t want to extend it. The employer usually communicate in some way with the employees to let them know that the contract is over and they should leave. Although sometimes it becomes tough, especially when you have other responsibilities and family to take care of, but life is life and it waits for no one. So, keep up your high spirit and try to find your next exciting job and move on.   20. Amendments   Amendments or appendices are usually internal rules, NDA and other legal documents attached to the main contract. These amendments usually follow the contract and you should sign them, too. Almost always, each amendment has at least one reference in the main contract. So, when reviewing the contract, request accompanying documents, a.k.a. amendments, and review them, too. There are many details in the main contract and the amendments, so try to put enough time to review them and then decide what to do. Seeking the help of a professional is highly recommended, in case you don\u2019t understand some parts of it. Googling about it is also good and would enlighten you even more.   \ud83d\uddf9 The Decision   Three situation would occur after carefully reviewing the contract and its amendments:   1. Negotiate   You are not happy or are confused with one or more parts of the contract and you want to change it, like salary, benefits or other terms and conditions. Go forward and tell them that you want to negotiate and seek clarification on them. Don\u2019t hesitate to negotiate, since losing an offer is better than having a bad contract.   2. Sign   If you\u2019ve finally decided to sign, now is the best time to do it. Go thank them that they offered you this, and sign it. May you leave happily ever after.   3. Reject   You have chosen not to accept the offer or you may have been offered a more luxurious contract from another company. Go thank them that they offered you this, and kindly reject it. May you leave happily ever after.   Last but not least, I do highly appreciate your feedback on this article and would like to hear about it. Hope you enjoyed reading the article as much as I enjoyed writing it.  ",categories:["Employment","Contracts","Law"],tags:["Employment","Contracts","Law"],url:"https://mostafa.dev/blog/employment-contract",teaser:null},{title:"Migration of Old Projects to Python 3",
excerpt:"A while ago I\u2019ve self-assigned the daunting task of migrating an ancient API project written in Python 2.7 and Django to the latest stable Python 3.7 and Django 2.2. What at first seemed to be a piece of cake, turned out to be a Hydra, the multi-headed dragon, with its dependencies acting like an octopus.   I should admit that it was no easy task. I remember days I was banging my head to get that tiny fix to work as expected. But it all went well, hopefully, and I was able to migrate the project into an almost stable piece of code that just works.   As far as I know, the countdown of sunsetting of Python 2.7 is nearer than it appears. It sadly means that the version 2 flavor of Python won\u2019t be maintained beyond 1st January 2020, which is inevitable. This date would be the official end of life of Python 2. This calls to an action to try to avoid it in the very near future for all companies using it.   The maintainers and the community worked hard to make everything compatible with the new Python 3 counterpart. The readiness score of Python 3 packages compatibility is eye-catching and a growing number of packages already has support for both version, while some completely switched to Python 3, specially those libraries written for newly widespread platforms, like shiny new database drivers. There is a timeline for many important packages that will Python 3, and would drop support for the old Python 2.   Writing code against both flavors of Python and maintaining that code-base is really hard, since the maintenance cost is high. It is even more costly when you want to develop a library, since actual projects can stick with a specific flavor. There is always a trade-off between choosing the old version and having the latest greatest features. This being said, so much can be ported to the old version, and eventually portability becomes an issue itself. For almost every single package you find in Python 3, either in the standard library or in third-parties, there is a backport to Python 2, but this doesn\u2019t hold true for the new features of Python 3. Two simple example would be the very great asyncio and aiohttp libraries.   As I said above, migration is no easy task, meaning that you cannot just change your interpreter version in your configuration file and headers and expect it to work right away. You should plan, analyze and iterate through the code-base to have everything migrated. I\u2019ll explain each in greater details.   In the middle of doing this, I felt the need to have some search mechanism that can understand the structure of the Python code, so that I\u2019ll be able to search inside imports in each file in the project, in order to able to understand where each module is actually used. So I\u2019ve written a script that maps all python files inside the project to a nested dictionary, with its values being imports in that specific file. It then gives me the ability to search with a XPath-like syntax (via dpath). I\u2019ll make it available soon as FOSS. It\u2019s called \u201cAlgae\u201d.   Methodology      Planning involves knowing the extent you wish your project to be updated. Sometimes you want to stick with an LTS version of some library and other times you want the latest cutting-edge features. This trade-off of stability versus trendiness is what to look for while planning for the migration.   This planning should also take into account the business side, although the focus of this article is on the engineering complications. Some changes should be organized and changed with the help of different departments. Otherwise it just won\u2019t work. You\u2019re going to deliver the same value or more to the customer, while decreasing technical debt and maintenance costs.   Analysis is the next step. You probably are already familiar with the project you are working on, or at least have worked on some parts of it and you know what it does. Analysis is to know what to keep, update or get rid of. Sometimes the project has grown organically and you no longer use some parts of it, like old API versions. Now is a good time to rethink. Sometimes a rewrite would be a more viable option, since the code-base is so messy that you no longer want to touch it. Other times, polishing and migration of the same code-base is easier and less costly. Whichever path you take would reveal your next moves.   Some changes include update to the backend services, like an update to a database driver that no longer supports outdated versions of that database. Some would break the tests, undoubtedly. This is what to expect while analyzing your code-base. You can\u2019t just update the code and expect it to work without touching anything else. You have to think about it thoroughly.   Iteration is the actual work you want to do to ensure that you have an up-to-date application. It is called iteration, since you cannot move everything all at once. It is truly infeasible. Just don\u2019t!   On each iteration, you actually take action by changing each part meticulously. Some parts are easy to change and require minimal amount of work, like updating small functional dependencies. Yet there are parts that require a lot of effort and time to change and test.   Recommendations   1. Use branching to isolate changes   Each change should go into a new branch, e.g. git branch. This is to decrease clutter and friction with other parts. This would help narrow down the scope of change and would help you easily revert breaking or malfunctioning changes.   It is okay to build on top of the same change by sub-branching from the same branch you used to work on, and which is tested and worked as expected. You cannot push people to review your code changes, although being subjective, except the company\u2019s future depends on it.   2. Upgrade everything to latest version supported by Python 2.7   The first iteration, besides version control and branching, would be to update dependencies to the latest version supported by Python 2.7. This helps find some bugs and alleviate many headaches afterwards. For example, the latest long-term support (LTS) version of Django supporting Python 2.7 is 1.11.x and of Django REST Framework is 3.9.x. This means that the official support for these libraries on Python 2.7 is dropped and no longer available. It\u2019s safe to say that upgrading to these versions has the least friction, unless your existing Django and DRF is ancient and unsupported.   Sometimes the changes in each version of Django and/or DRF is not backward compatible, so you have to keep an eye on Release Notes and Changelogs to adapt your code to the new changes.   3. Monitor tests, Continuous Integration and coverage   In this step, you should ensure that your tests pass and your coverage is not decreased dramatically. One of the biggest technical debts are not having enough tests, which would come to surface with users complaining about failures and developers seeing flaky test runs while updating code.   If that\u2019s the case, consider writing more tests and invest more on CI workflows and pipelines. Of investment, I don\u2019t necessarily mean money. Sometimes correcting a broken pipeline would help alleviate many issues in the future.   The next step is to monitor your test results to see if there are errors and warnings related to your code. Yes, I\u2019ve seen tests pass, having many errors and warnings that are simply ignored. Test runner should be chosen wisely. Although the \u201cunittest\u201d standard library module is feature-rich, I do prefer \u201cpytest\u201d as a more flexible and extendable counterpart. This obviously doesn\u2019t stop you from choosing your preferred test suite or even writing your own.   Your tests should not only include unittests. Integration, E2E, acceptance and performance test are just some of all the available test methodologies.   4. Run 2to3 on your code-base   After upgrading to the latest versions supported on Python 2.7 and testing to see if everything\u2019s in order, now is the time to run the 2to3 tool on your code-base. This is a tool provided as part of the Python standard library to read the Python 2.x source code and transform them into a valid Python 3.x source code, through the so-called fixers. Fixers are changes to the syntax and semantics of the source code from Python 2.x to 3.x. For example, in Python 2.x, it is not necessary to have parenthesis around the print function, but such functions no longer exist in Python 3.x. So the print fixer converts print function to print() all over your source code. Test again and again and make sure not to mess things up by not using branching.   Update: a user on reddit commented that we could use modernize instead, to be able to spot more conversion errors, specially Unicode-related ones.   5. Change your interpreter and package manager   Now is the time to change your Python version to 3.7. Usually this is done by either installing it using a package manager, like APT, or just changing your Dockerfile FROM statement to include python:3.7.   This is the trickiest part, since many tests may fail, due to incompatible requirements and dependencies.   I do also recommend you to upgrade your package manager, pip, to the latest version.   6. Upgrade your dependencies   It is a good idea to check your dependencies before upgrade by running pipdeptree on the requirements.txt file to detect circular and conflicting dependencies. The next best tool is the pip-update-requirements that helps you upgrade all dependencies in the requirements.txt file.   Some dependencies are not easily upgradable, either because they are no longer supported anymore, specially on Python 3.x, or newer libraries have replaced them. For example, pycassa is an old database driver for Cassandra supporting only Python 2.x, with its last commit on 17 January 2017, which only supports Thrift protocol and there is no CQL support. But the official DataStax Python Driver replaced it deliberately supporting 2.7 and 3.x, which is easier to use and has more features and also is based on CQL.   Some other libraries have different issues. For example, python-social-auth package is deprecated and you should upgrade all your dependencies to social-auth-core and social-auth-app-django, in which you have to change many parts of the settings.py and your imports.   Some other libraries are replaced with better counterparts. For example, the python-memcached library has many other more up-to-date alternatives like pymemcache or pylibmc.   7. Upgrade Django and DRF to latest version   The Django release process is a good resource for knowing what to use and when to deprecate and get rid of old versions. The release roadmap on Django download page will give you a very quick view into the lifetime of each version. This roadmap is frequently updated as releases become available. Below you can see two different version:         Credits: https://www.djangoproject.com/download/   As of now, the latest LTS version of Django that only supports Python 3.x is 2.2.x and the latest version of DRF is 3.10.x.   If you could eventually pass step 6, you can almost easily upgrade these versions and test if everything holds together.   Upgrading Django from 1.11.x to 2.x is a lot more painful than I thought it would be and it needs more time to be invested to get everything work correctly. There are removals, deprecations and changes that you should be aware of, to not break your system. Consider doing the upgrade step by step, otherwise you\u2019ll be in big trouble hunting multiple unrelated bugs and the whole process would quickly become exhausting.   I hope you got an overview of what to expect while upgrading your Django and DRF application to the latest versions supported by Python 3.7. If you have any questions, comments or improvements, I\u2019ll be really glad to hear them.  ",categories:["Python","Django","Django Rest Framework","Migration","Methodology"],tags:["Python","Django","Django Rest Framework","Migration","Methodology"],url:"https://mostafa.dev/blog/migration-of-old-projects-to-python-3",teaser:null},{title:"Integrating k6 with Apache Kafka",excerpt:'Undoubtedly, Apache Kafka is one of the most prominent pieces of software existing in today\u2019s distributed architectures, from cloud-providers supporting it to on-premise setups. It can be used to store and process millions of messages per second, so it makes it a perfect fit for distributed real-time data processing solutions. Although it provides very advanced features like stream-processing and horizontal scalability, it is very easy to use: you just have to send your messages to it (produce) and later pick them up (consume) for further processing. It has all the features you need from a modern distributed data store: communication interface, persistence, scalability, streaming and management. Once developed at LinkedIn for data processing, later donated to the Apache Software Foundation. I do highly recommend you to have a look at its documentation.   k6 OSS is an open-source load testing tool built with unit-testing in mind, but for performance. It can produce hundreds of messages per second, as a result of testing a system or platform. These messages can be sent to a number of different destinations, like a JSON file or Apache Kafka, which works out of the box with zero dependency, other than the Apache Kafka setup itself.   To be able to send messages from k6 to Apache Kafka, we\u2019d go with a very simple setup, with only one instance. In our scenario, k6 would act as a simple producer of JSON messages and we\u2019ll try to consume message on terminal, using the built-in simple kafka consumer. Here I will use the official binary provided by Apache and would leave the docker setup up to you to try. To give you a clue, using Lenses.io\u2019s fast-data-dev, you can have a complete Docker environment with Kafka, ZooKeeper, Schema Registry, Kafka-Connect, Landoop Tools and more than 20 connectors in an easy to use package.   Since we\u2019re using the binary package, no special installation is needed, and you can directly invoke the binary from the command line. I have used Debian GNU/Linux 10.2 to run these commands. Simply follow the instructions below:   1. Download and extract Apache Kafka platform binaries   The current version as of this writing is 2.3.0, but you can choose your own desired version.   $ wget http://apache.mirrors.spacedump.net/kafka/2.3.0/kafka_2.12-2.3.0.tgz $ tar xvf kafka_2.12-2.3.0.tgz $ cd kafka_2.12-2.3.0   2. Start Apache ZooKeeper and Kafka   As you can see, when you start the ZooKeeper, it will start listening on all interfaces (0.0.0.0) on port 2181.   $ bin/zookeeper-server-start.sh config/zookeeper.properties   Now start Kafka server in a new terminal. It will connect to your local ZooKeeper instance on port 2181 and will start listening for new connections on port 9092.   $ bin/kafka-server-start.sh config/server.properties [2019-11-15 12:43:54,672] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer) ... [2019-11-15 12:43:55,366] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor) ... [2019-11-15 12:43:55,666] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)   3. Create a topic to send k6 data to   On another terminal, create a topic, for example k6-output, to receive k6 messages on. This topic would be a single partition topic with no replication.   $ bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic k6-output   4. Start k6 with Kafka output   Using the following script in a file named scenario.js, we\u2019ll start k6 to send its output to Kafka:   import { check, sleep } from "k6"; import http from "k6/http";  export let options = {   duration: "1m",   vus: 100 };  export default function() {   let res;   res = http.get("https://httpbin.org/get");   check(res, { "status is 200": r =&gt; r.status === 200 });    res = http.get("https://httpbin.org/bearer", {     headers: { Authorization: "Bearer da39a3ee5e6b4b0d3255bfef95601890afd80709" }   });   check(res, {     "status is 200": r =&gt; r.status === 200,     "is authenticated": r =&gt; r.json()["authenticated"] === true   });    res = http.get("https://httpbin.org/base64/azYgaXMgYXdlc29tZSE=");   check(res, {     "status is 200": r =&gt; r.status === 200,     "k6 is awesome!": r =&gt; r.body === "k6 is awesome!"   });    sleep(1); }   $ k6 run --logformat=raw --out kafka=brokers=localhost:9092,topic=k6-output,format=json scenario.js   5. Consume messages on a terminal   To view messages of k6 output on your terminal, use the following built-in consumer:   $ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic k6-output   As you can see, we\u2019ve sent our messages with JSON format to Kafka, so you can write a simple JSON consumer in your own desired programming language (e.g. Python) and start consuming and using the data provided by k6 to Kafka. There are other possibilities, like using platforms that can import or consume data from Apache Kafka, like Apache Spark or Kafka Connect from Confluent Platform, to name a few.   You can also use visualization platforms that has support for Kafka as input. I recommend reading the following articles about it:   Just Enough Kafka for the Elastic Stack, Part 1   Just Enough Kafka for the Elastic Stack, Part 2   Guest Blog Post: How the k6 Load Testing Tool Is Leveraging Grafana   I hope you\u2019ve found this article helpful and were able to send your k6 output to Apache Kafka, an industry-proven technology.   If you have any suggestions or comments, I really like to hear from you.  ',categories:["k6","Load Impact","Apache","Kafka","Integration"],tags:["k6","Load Impact","Apache","Kafka","Integration"],url:"https://mostafa.dev/blog/integrating-k6-with-apache-kafka",teaser:null},{title:"API & Backend Security",
excerpt:"In my previous article about secure code review and penetration testing of Node.js and JavaScript apps, I tried to give the reader an introduction to the old, but gold, OWASP Code Review Project and the OWASP Top 10 Project and to mention the tools of the trade using the simplest approach possible.   Recently the new OWASP API Security Top 10 2019 is released, as a release candidate, which is a breath of fresh air into the field of security and penetration testing. It is a tribute to its old pal, OWASP Backend Security Project, which was released in 2008, or at least, I see it so, as the naming suggests. But deep down, they are two different guides and two different approaches to security. This new one deals with the Top 10 approach using scenarios, while the old one tries to present a multi-phase approach, with phases being development, hardening and testing.   In this article I will try to present the new one using all the current tools and techniques available to defend against threats to your APIs. I will focus on Python, since it is my area of expertise, and would present packages and tools either directly or generally related to it.   It is really interesting to see how the Top 10 list of software vulnerabilities of an API is different from the original Top 10 list, priority-wise. This sheds a light on the importance of different measures to take.   The following is the current Top 10 list of vulnerabilities presented in the document, which I will go through one by one afterwards:      API1:2019-Broken Object Level Authorization  API2:2019-Broken Authentication  API3:2019-Excessive Data Exposure  API4:2019-Lack of Resources &amp; Rate Limiting  API5:2019-Broken Function Level Authorization  API6:2019-Mass Assignment  API7:2019-Security Misconfiguration  API8:2019-Injection  API9:2019-Improper Assets Management  API10:2019-Insufficient Logging &amp; Monitoring    API1:2019 - Broken Object Level Authorization   As the title implies, it is all about giving unwanted parties or attackers extra unnecessary access to information. The AAA (Authentication, Authorization and Accounting) of network protocols also apply to high-level applications sitting on top of the Application Layer (L7) in the TCP/IP network protocol stack, in which, in this case, the Access would be interchangeably used instead of Accounting. Although the vulnerability only mentions authorization, I think that other things, like authentication that precedes and access that succeeds it, should also be explored, which I\u2019ll do below.   \u2705 Authentication   Authentication is a mechanism by which one\u2019s identity is verified. This means that they have been either identified or unidentified, in advance. This ensures that no one can or will take anyone else\u2019s identity. In applications, identity management is easily described by user management, in which, each user/actor of the system is considered an identity known to the system, which therefore can be verified through a mechanism or a protocol.   Usually authentication happens via a mechanism or protocol, either application-based or protocol-based, working in different networking stack layers. An example of application-based authentication is the user authentication module in Django, which usually uses a ModelBackend* *to filter through a list of users (by username or email) and their matching password existing in a database or a data store. Authentication protocols are either application-layer based ones, like digest access authentication, or they operate on lower layers in the stack, e.g. IEEE 802.1X, though all of which are considered and categorized under SASL framework.   These are the things to consider while performing any kind of authentication and authorization and granting access:      NEVER EVER TRUST USER INPUT! **\u2019 or \u20181\u2019=\u20191**      https://github.com/marshmallow-code/marshmallow                                                                               marshmallow-code/marshmallow                   A lightweight library for converting complex objects to and from simple Python datatypes. - marshmallow-code/marshmallow                        https://github.com/              https://github.com/marshmallow-code/webargs                                                                               marshmallow-code/webargs                   A friendly library for parsing HTTP request arguments, with built-in support for popular web frameworks, including Flask, Django, Bottle, Tornado, Pyramid, webapp2, Falcon, and aiohttp. - marshmall...                        https://github.com/              https://github.com/samuelcolvin/pydantic                                                                               samuelcolvin/pydantic                   Data parsing and validation using Python type hints - samuelcolvin/pydantic                        https://github.com/              https://github.com/pyeve/cerberus                                                                               pyeve/cerberus                   Lightweight, extensible data validation library for Python - pyeve/cerberus                        https://github.com/              https://docs.djangoproject.com/en/2.2/ref/models/instances/#validating-objects                                    Model instance reference | Django documentation | Django                   The web framework for perfectionists with deadlines....                        https://docs.djangoproject.com/              https://github.com/Pylons/colander                                                                               Pylons/colander                   A serialization/deserialization/validation library for strings, mappings and lists. - Pylons/colander                        https://github.com/              https://github.com/keleshev/schema                                                                               keleshev/schema                   Schema validation just got Pythonic. Contribute to keleshev/schema development by creating an account on GitHub.                        https://github.com/              https://github.com/schematics/schematics                                                                               schematics/schematics                   Python Data Structures for Humans\u2122. Contribute to schematics/schematics development by creating an account on GitHub.                        https://github.com/              https://github.com/podio/valideer                                                                               podio/valideer                   Lightweight data validation and adaptation Python library. - podio/valideer                        https://github.com/              https://github.com/alecthomas/voluptuous                                                                               alecthomas/voluptuous                   CONTRIBUTIONS ONLY: Voluptuous, despite the name, is a Python data validation library. - alecthomas/voluptuous                        https://github.com/              https://github.com/Julian/jsonschema                                                                               Julian/jsonschema                   An(other) implementation of JSON Schema for Python - Julian/jsonschema                        https://github.com/              https://github.com/wesbos/burner-email-providers                                                                               wesbos/burner-email-providers                   A list of temporary email providers. Contribute to wesbos/burner-email-providers development by creating an account on GitHub.                        https://github.com/              Authenticate properly, either via SSO, SAML, social authentication or whatever method you choose to authenticate against!      https://github.com/python-social-auth/social-core                                                                               python-social-auth/social-core                   Python Social Auth - Core. Contribute to python-social-auth/social-core development by creating an account on GitHub.                        https://github.com/              https://github.com/davesque/django-rest-framework-simplejwt                                                                               SimpleJWT/django-rest-framework-simplejwt                   A JSON Web Token authentication plugin for the Django REST Framework. - SimpleJWT/django-rest-framework-simplejwt                        https://github.com/              https://github.com/pennersr/django-allauth                                                                               pennersr/django-allauth                   Integrated set of Django applications addressing authentication, registration, account management as well as 3rd party (social) account authentication. - pennersr/django-allauth                        https://github.com/              https://github.com/IdentityPython/pysaml2                                                                               IdentityPython/pysaml2                   Python implementation of SAML2. Contribute to IdentityPython/pysaml2 development by creating an account on GitHub.                        https://github.com/              https://github.com/onelogin/python-saml                                                                               onelogin/python-saml                   Python SAML Toolkit. Contribute to onelogin/python-saml development by creating an account on GitHub.                        https://github.com/              Always use a rate limiting mechanism to disallow users from using a single pair of username and password to authenticate multiple time in a row. This helps prevent brute-force attacks and credential stuffing.      https://github.com/tomasbasham/ratelimit                                                                               tomasbasham/ratelimit                   API Rate Limit Decorator. Contribute to tomasbasham/ratelimit development by creating an account on GitHub.                        https://github.com/              https://github.com/RazerM/ratelimiter                                                                               RazerM/ratelimiter                   Simple Python module providing rate limiting. Contribute to RazerM/ratelimiter development by creating an account on GitHub.                        https://github.com/              https://github.com/alisaifee/flask-limiter                                                                               alisaifee/flask-limiter                   Rate Limiting extension for Flask . Contribute to alisaifee/flask-limiter development by creating an account on GitHub.                        https://github.com/              https://github.com/jsocol/django-ratelimit                                                                               jsocol/django-ratelimit                   Cache-based rate-limiting for Django. Contribute to jsocol/django-ratelimit development by creating an account on GitHub.                        https://github.com/              https://www.django-rest-framework.org/api-guide/throttling/                                    Throttling - Django REST framework                   HTTP/1.1 420 Enhance Your Calm...                        https://www.django-rest-framework.org/              https://github.com/jazzband/django-axes                                                                               jazzband/django-axes                   Keep track of failed login attempts in Django-powered sites. - jazzband/django-axes                        https://github.com/              https://github.com/jazzband/django-defender                                                                               jazzband/django-defender                   A simple super fast django reusable app that blocks people from brute forcing login attempts - jazzband/django-defender                        https://github.com/              https://github.com/brutasse/django-ratelimit-backend                                                                               brutasse/django-ratelimit-backend                   Rate-limit your login attempts at the authentication backend level - brutasse/django-ratelimit-backend                        https://github.com/              Make sure you keep track of IP address users are authenticating from, and limit users authenticating numerous times via the same IP address, thus preventing the same brute-force attack and also preventing man-in-the-middle attacks.   Always use the most secure protocol or mechanism to authenticate users. There is always a trade-off between security and usability, so beware!   Authentication tokens are not as secure as they seem to be, since stolen tokens are the key to everything. So, always use some sort of expiry, like refreshing the token every once in a while.      https://auth0.com/blog/critical-vulnerabilities-in-json-web-token-libraries/                                                                               Critical vulnerabilities in JSON Web Token libraries                   Which libraries are vulnerable to attacks and how to prevent them.                        https://auth0.com/              https://connect2id.com/products/nimbus-jose-jwt/vulnerabilities                                    Common JWT security vulnerabilities and how to prevent them | Connect2id                    The JSON Web Token has received a number of security reviews at the IETF and OIDF and is deemed sufficiently secure by experts. But this doesn't make it foolproof. You, as a develop...                        https://connect2id.com/              https://www.nccgroup.trust/uk/about-us/newsroom-and-events/blogs/2019/january/jwt-attack-walk-through/                                    JWT Attack Walk-Through                   There\u2019s a well-known defect with older versions of certain libraries where you can trick a JSON Web Token (JWT) consumer that expects tokens signed using asymmetric cryptography...                        https://www.nccgroup.com/              Use end-to-end encryption.      https://github.com/pyca/cryptography                                                                               pyca/cryptography                   cryptography is a package designed to expose cryptographic primitives and recipes to Python developers. - pyca/cryptography                        https://github.com/              https://github.com/dlitz/pycrypto                                                                               pycrypto/pycrypto                   The Python Cryptography Toolkit. Contribute to pycrypto/pycrypto development by creating an account on GitHub.                        https://github.com/              Use strong hashing mechanism that support better randomness and salt.   Firewalls are dumb. Use them, but don\u2019t trust them fully. The same applies for other defensive mechanisms.   Misconfigurations are the root of all evil. Take it seriously!   Use captcha and account lockout mechanisms, which is also mentioned above.   Never permit weak passwords!   Never send authentication details in URL!   Always verify tokens, in terms of validity, expiry, etc!   Never reply with meaningful sensitive information. For example, replying whether a user exists or not to a forget password request will help the attacker enumerate users existing on your API or website. Ambiguous messages and 204 No Content status code help prevent this issue.   Follow the guidelines in OWASP Authentication Cheatsheet.   API keys are for projects, authentication is for users.   Use multi-factor authentication (MFA), where possible, but also beware of SIM swap attack which may cause issues:      https://www.wired.com/story/sim-swap-attack-defend-phone/                                                                               How to Protect Your Phone Against a SIM Swap Attack                   Your phone number is increasingly tied to your online identity. You need to do everything possible to protect it.                        https://www.wired.com/              https://hackernoon.com/my-sim-swap-attack-how-i-almost-lost-dollar71k-and-how-to-prevent-it-tj39q3aju                                    My SIM swap attack: How I almost lost $71K, and how to prevent\xa0it | Hacker Noon                   Technical marketer with startup experience; co-founder of blockchain platform Provide Technologies....                        https://hackernoon.com/           \u2705 Authorization   The second step is authorization, in which, the system decides whether the authenticated user is authorized to access a resource. Imagine you have a website selling stuff. You don\u2019t want each user to be able to see what other users have ordered, well, for privacy reasons. So the object of concern here is the order history and only the owner of the order(s) can view it. If by some mechanism, other users can view each others\u2019 order history, this makes your website insecure and untrustable.   In terms of an API, you should disallow unauthorized access to resources via some authorization mechanism. As is practiced, it is best to authorize users based on their access to objects, and hence models. I have mentioned models, just because nowadays it is widely accepted to use an OxM (Object-to-X-Mapper, like ORM, OGM, ODM, etc.) to access your database tables, rows, nodes, relations, documents or the like. Since models are the single source of truth, it would be good to authorize users based on them, not the routes, since a users may access a route they are not authorized to, hence jeopardizing the security of your API. This may seem a little bit complicated, but just keep it mind that authorizing based on models are easier and safer. You can do both, but it easily gets messy and hard to manage.   The usual method is to have a \u201cUser \u2194 Role \u2194 Permission\u201d set of models, in which authentication and user profile management is managed by the user object, and each user can have one or multiple role(s), which in turn, can have certain permissions granted to it. This is usually referred to as role-based access controls (RBAC).   Another method is to have a \u201cUser \u2194 Group \u2194 Permission\u201d set of models, in which a user belongs to one or more group(s) that may have a certain number of permissions. In this method, the group may have an arbitrary set of permissions, completely unrelated to the concept of its designation or role.   The hardest part is when one has more than one role or belongs to more than one group. In this case, the best approach is to take advantage of the principle of least privilege (PLoP), meaning that the lowest permission(s) always win, or simply put, a user doesn\u2019t need to have administrative access to view his order history. Also, make sure that you understand the object capability model to its fullest.      https://www.beyondtrust.com/blog/entry/what-is-least-privilege                                                                               What Is Least Privilege &amp; Why Do You Need It? | BeyondTrust                   Least privilege is the concept and practice of restricting access rights for users, accounts, and computing processes to only those resources absolutely required to perform routine, legitimate activities. This blog provides an in-depth overview of least privilege.                        https://www.beyondtrust.com/              https://github.com/casbin/pycasbin                                                                               casbin/pycasbin                   An authorization library that supports access control models like ACL, RBAC, ABAC in Python - casbin/pycasbin                        https://github.com/              https://github.com/YosaiProject/yosai                                                                               YosaiProject/yosai                   A Security Framework for Python applications featuring Authorization (rbac permissions and roles), Authentication (2fa totp), Session Management and an extensive Audit Trail - YosaiProject/yosai                        https://github.com/              https://github.com/shonenada/flask-rbac                                                                               shonenada/flask-rbac                   Flask-RBAC. Contribute to shonenada/flask-rbac development by creating an account on GitHub.                        https://github.com/              https://github.com/dimagi/django-prbac                                                                               dimagi/django-prbac                   Contribute to dimagi/django-prbac development by creating an account on GitHub.                        https://github.com/              https://github.com/klada/django-auth-rbac                                                                               klada/django-auth-rbac                   An attempt of implementing role-based access control for Django - klada/django-auth-rbac                        https://github.com/              https://github.com/django-guardian/django-guardian                                                                               django-guardian/django-guardian                   Per object permissions for Django. Contribute to django-guardian/django-guardian development by creating an account on GitHub.                        https://github.com/              https://github.com/sunscrapers/djoser                                                                               sunscrapers/djoser                   REST implementation of Django authentication system. - sunscrapers/djoser                        https://github.com/              https://github.com/dfunckt/django-rules                                                                               dfunckt/django-rules                   Awesome Django authorization, without the database - dfunckt/django-rules                        https://github.com/              https://github.com/mgrouchy/django-stronghold                                                                               mgrouchy/django-stronghold                   Get inside your stronghold and make all your Django views default login_required - mgrouchy/django-stronghold                        https://github.com/              https://github.com/jazzband/django-authority                                                                               jazzband/django-authority                   A Django app that provides generic per-object-permissions for Django's auth app and helpers to create custom permission checks. - jazzband/django-authority                        https://github.com/           \u2705 Access   Sometimes you have the user authenticated and authorized to access a resource, but the resouce itself needs some special permissions, like delete permissions. Users may be able to view their order history, but may (or should) not be able to change them. This boils down to the specific object-level permissions one resource may have.   So, authorization is the permission(s) the user has, while access is the permission(s) a resource needs, for the system to be able to grant access to it.   This subtle difference is usually disregarded or misinterpreted, which leads to the topic of the current vulnerability I am talking about, broken object level authorization. From the user\u2019s perspective, when they are in, they are in, but from the system\u2019s and security perspective, this may not be the case. So, try to implement fine-grained access controls to restrict users and attackers from gaining access to your valuable resources.   API2:2019 - Broken Authentication   This topic is well discussed above, so if you want to know more about this vulnerability, make sure you read the first section, API1:2019-Broken Object Level Authorization.   API3:2019 - Excessive Data Exposure   It is considered a bad design decision, security-wise, to rely on client to filter out data returned by API, since an attacker can sit in between and hear all the communications (requests and responses) and sniff out sensitive information that should not be normally present.   Always review all response data returned by all endpoints and try to validate all returned data against a schema to prevent exposure. Examples of this is returning all user objects data to the client, like password hashes and other sensitive data.   API4:2019 - Lack of Resources &amp; Rate Limiting   Rate limiting is already discussed in API1:2019-Broken Object Level Authorization to prevent brute-force attacks. It also applies to other system resources like CPU, memory, etc., which is always scarce, no matter how much you have and it can be easily saturated by overuse. Scarcity makes for different strategies for resource management. Rate limiting is one of those strategies. Another is to limit resource usage using something like docker that can control resource usage.   Relying on the client for data validation is a true mistake and should be prevented at all costs, meaning that client- and server-side validation should accompany each other. This also includes enforcing incoming data size to a specific amount.   There are cases like zip bombs that can explode your resources on the server, so just verify compression ratio of compressed files before accepting and extracting the file:      https://portswigger.net/daily-swig/ancient-zip-bomb-attack-given-new-lease-of-life                                                                               Ancient ZIP bomb attack given new lease of life                   Decompression could give modern web apps a touch of the bends                        https://portswigger.net/              https://www.vice.com/en_us/article/597vzx/the-most-clever-zip-bomb-ever-made-explodes-a-46mb-file-to-45-petabytes                                                                               The Most Clever 'Zip Bomb' Ever Made Explodes a 46MB File to 4.5 Petabytes                   Files so deeply compressed that they\u2019re effectively malware have been around for decades\u2014and a researcher just unveiled a brand-new Zip bomb that explodes a 46-megabyte file to 4.5 petabytes of data.                        https://www.vice.com/              https://github.com/danielmiessler/SecLists/tree/master/Payloads/Zip-Bombs                                                                               danielmiessler/SecLists                   SecLists is the security tester&#39;s companion. It&#39;s a collection of multiple types of lists used during security assessments, collected in one place. List types include usernames, passwords, ...                        https://github.com/              https://github.com/twbgc/sunzip                                                                               twbgc/sunzip                   Provide secure unzip against zip bomb :bomb:. Contribute to twbgc/sunzip development by creating an account on GitHub.                        https://github.com/           API5:2019 - Broken Function Level Authorization   We have already discussed object level authorization in API1:2019-Broken Object Level Authorization, which deal with unauthorized access to objects. Current vulnerability discusses a method by which the attacker is already authorized or is using a malicious input against an endpoint that requires little or no authorization, like anonymous users\u2019 endpoints.   A small change to a HTTP request method would make the endpoint vulnerable. For example, changing a GET request to POST would make the endpoint create or update a record with the same anonymous access rights. This also applies to the request URL, where you can simply change a /users to /admins and you\u2019ll get all data relating to privileged admin users, which is horrible. Just keep in mind that almost everything in the request can be changed. Burp Suite is just one example, by which you can easily act as a proxy between client and the API and change the requests in between.      https://support.portswigger.net/customer/portal/articles/1783101-how-to-use-burp-suite                                                                               How to Use Burp Suite                   How to Use Burp Suite Burp Suite is an integrated platform for performing security testing of web applications. It is designed to be used by hands-on ...                        https://portswigger.net/           The best mechanism to prevent most of these vulnerabilities is to tighten enforcement of authorization rules, by denying everything by default and then explicitly giving access rights to specific roles for access to every function. This is also one of the most important topics in firewall configuration, in which the last rule is to deny all, in case that no rule matches against the criteria of the packet, is is denied and obviously not passed through.   API6:2019 - Mass Assignment   As said before, **NEVER EVER TRUST USER INPUT! **I just can\u2019t put more emphasis on this advice. Certain things in database and workflow of the API is managed by user input, like a user profile or metadata passed along with an upload request. Binding users\u2019 input parameters to exact objects without any validation and filtering is a recipe for disaster. Imagine an attacker sets a simple is_admin flag to True while sending a request to update a user\u2019s profile. If not taken seriously, it would make the attacker an admin in the system right away. Just take input validation seriously, so that the next attacker or curious user not be able to update his account\u2019s balance by updating it via a legitimate request to the system. Also be aware of some fields, like created and updated fields, which should almost always be internally handled and not passed from the user directly.   Use white- and black-list feature to prevent some fields from being updated via requests.      https://docs.djangoproject.com/en/2.2/ref/validators/                                    Validators | Django documentation | Django                   The web framework for perfectionists with deadlines....                        https://docs.djangoproject.com/              https://www.django-rest-framework.org/api-guide/permissions/                                    Permissions - Django REST framework                   Authentication or identification by itself is not usually sufficient to gain access to information or code.  For that, the entity requesting access must have authorization....                        https://www.django-rest-framework.org/              https://github.com/dbrgn/drf-dynamic-fields                                                                               dbrgn/drf-dynamic-fields                   Dynamically select only a subset of fields per DRF resource, either using a whitelist or a blacklist. - dbrgn/drf-dynamic-fields                        https://github.com/           API7:2019 - Security Misconfiguration   As I said above, misconfigurations, specially security misconfigurations, are the root of all evil. Taking security configurations as granted is a grave mistake. Since security is always considered as an afterthought, try to get rid of the temptation to forego implementing security recommendations, like encryption, hashing, rate-limiting, CORS, TLS, etc.   There are some general pieces of advice:      Integrate hardening process in your workflow.   Regularly review and update configurations and packages on your projects and infrastructure. Always apply the latest security patches.   Use E2E encryption!   Try to integrate automated configuration analysis and vulnerability assessment in your workflow.   Never send stack traces produced by exceptions to the user (or potential attacker). Always validate the response against a schema.   Static analysis and linting will detect most of misconfigurations and bugs in the first place.      https://docs.djangoproject.com/en/2.2/topics/security/                                    Security in Django | Django documentation | Django                   The web framework for perfectionists with deadlines....                        https://docs.djangoproject.com/              https://github.com/mattupstate/flask-security                                                                               mattupstate/flask-security                   Quick and simple security for Flask applications. Contribute to mattupstate/flask-security development by creating an account on GitHub.                        https://github.com/              https://snyk.io/                                                                               Snyk | Develop Fast. Stay Secure                   Snyk helps you use open source and stay secure. Continuously find and fix vulnerabilities for npm, Maven, NuGet, RubyGems, PyPI and much more.                        https://snyk.io/              https://www.sqreen.com/                                                                               Application Security Management Platform | Sqreen                   Learn more about Sqreen's application security platform that helps teams protect applications, increase visibility and secure code.                        https://www.sqreen.com/              https://detectify.com/                                                                               Leading website vulnerability scanner | Free 14 day trial                   Web security issues are a major pain, thankfully our website vulnerability scanner identifies issues before they become a problem. Find vulnerabilities before hackers do!                        https://detectify.com/              https://arvan.cloud/                                                                               ArvanCloud | Integrated Cloud Infrastructure                   ArvanCloud offers integrated cloud services like CDN, Cloud DNS, Cloud Security, Cloud datacenter, Cloud storage, VoD, Live streaming, and video ads.                        https://arvan.cloud/              https://semmle.com/                                                                               Semmle - Code Analysis Platform for Securing Software                   Semmle's code analysis platform helps teams find zero-days and automate variant analysis. Secure your code with continuous security analysis and automated code review.                        https://semmle.com/           API8:2019 - Injection   Injection, injection, injection! It is almost 2020 and we have have this injection issue! Neither OxMs nor any other pieces of software today can fully prevent it from being exploited. Well, there has been some advances in IDSes and cloud services to inspect and prevent such attacks, but it can still be exploited in the wide. Just keep in mind that NoSQL is also vulnerable to injection attacks.   Nowadays working without having any relation to a third-party API is just not feasible anymore. Things like payment service providers, authentication providers and the like are all examples of such services. We, as developers, usually tend to think that when we pay for something, is it to be trusted, but sometimes reverse proves to be the case! So, I suggest you not to trust any data coming from external systems and APIs and validate them, too! For example, an attack on a third-party API may propagate to those using it in their APIs.   These are some resources to learn more about ways to prevent injection:      https://realpython.com/prevent-python-sql-injection/                                                                               Preventing SQL Injection Attacks With Python \u2013 Real Python                   SQL injection attacks are one of the most common web application security risks. In this step-by-step tutorial, you'll learn how you can prevent Python SQL injection. You'll learn how to compose SQL queries with parameters, as well as how to safely execute those queries in your database.                        https://realpython.com/              https://blog.sqreen.com/preventing-sql-injections-in-python/                                                                               Preventing SQL injections in Python (and other vulnerabilities) - Sqreen Blog                   How to prevent SQL injections in Python. Article explains how to identify security vulnerabilities in Python and how to protect applications from attacks                        https://blog.sqreen.com/              https://www.owasp.org/index.php/Reviewing_Code_for_OS_Injection                                    OWASP Code Review Guide                   Virtual AppSec Days Summer of Security CFT is open! Apply Now...                        https://owasp.org/              https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/XML_External_Entity_Prevention_Cheat_Sheet.md#python                                                                               OWASP/CheatSheetSeries                   The OWASP Cheat Sheet Series was created to provide a concise collection of high value information on specific application security topics. - OWASP/CheatSheetSeries                        https://github.com/           API9:2019 - Improper Assets Management   APIs do not usually exist as a separate isolated entity, without having any relation or connection to its environment and outside world. They are literally living beings that sit there and listen to requests and eventually return a response. Yet they can be seen as assets to a company, making enough money for it to survive and grow. APIs and their belongings, like environments, accesses, \u2026, should be seen as an inventory and shoud all be well-documented and accounted for. In turn, roles in the system and its data flow should also be documented. Security policies also need their own documentation.   Versioning is an important part of product development, which is concretely seen as an important aspect of API development, where the routes themselves contain version information, e.g. /v1/products or /v2/consumers. Along the lifecycle of the API, some endpoints are become obsolete or deprecated, so they should be somehow decommissioned. Keeping track of these endpoints in older versions of the API help reduce the attack surface of the API, in case there is a gem for attackers hidden inside. At the same time, some vulnerabilities are found in the new endpoints which needs to be ported the old ones. This specific backports should be taken seriously, by QAs, reviewers and testers and be labeled separately in any project management or ticketing solution to provide more visibility into the matter.   API10:2019 - Insufficient Logging &amp; Monitoring   With enough logging and monitoring all issues can be unearthed and examined before causing any severe damage to the system (API) and credibility of the company.   It is really easy to saturate and overwhelm the log server with useless log messages. In terms of security, focus on special log messages, called security events, like failed authentication attempts, denied accesses and input validation errors and try to log enough data to help uncover the issue. Logs are considered as sensitive data, so handle it with care!   Implement and configure a monitoring solution to continuously monitor every asset in your system and use a Security Information and Event Management (SIEM) solution to aggregate logs and provide meaningful insights out of them.   Last but not least, do not stick with the default dashboards, customize them to show important security events first.   These are some recommendations, but you know best!      https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html                                    Logging                   This cheat sheet is focused on providing developers with concentrated guidance on building application logging mechanisms, especially related to security logging....                        https://cheatsheetseries.owasp.org/              https://grafana.com/                                                                               Grafana: The open observability platform                                           https://grafana.com/              https://www.elastic.co/what-is/elk-stack                                    ELK Stack: Elasticsearch, Logstash, Kibana | Elastic                   Add search to your app...                        https://www.elastic.co/              https://www.datadoghq.com/                                                                               Cloud Monitoring as a Service | Datadog                   See metrics from all of your apps, tools &amp; services in one place with Datadog's cloud monitoring as a service solution. Try it for free.                        https://www.datadoghq.com/              https://www.esecurityplanet.com/products/top-siem-products.html                                    Top SIEM Products                   Security Information and Event Management (SIEM) is a key enterprise security technology, with the ability to tie systems together for a comprehensive view of IT security.                        https://www.esecurityplanet.com/              https://www.capterra.com/siem-software/                                    Best SIEM Software | 2020 Reviews of the Most Popular Tools &amp; Systems                   Find and compare SIEM software. Free, interactive tool to quickly narrow your choices and contact multiple vendors.                        https://www.capterra.com/           Conclusion   What I\u2019ve tried to achieve in this long article was to inform you of the Top 10 list of vulnerabilities found in today\u2019s APIs that threatens their existence. I\u2019ve mentioned many Python packages, cheatsheets, related resources and software needed for the job. But the developer always knows best. Yeah, common sense can alleviate most of these.   Try to take advice from experts in the field, specially those contributed to the making of the Top 10 list and NEVER EVER TRUST USER INPUT!  ",
categories:["OWASP","Top 10","API","Security","Python"],tags:["OWASP","Top 10","API","Security","Python"],url:"https://mostafa.dev/blog/api-backend-security",teaser:null},{title:"microVM: Another Level of Abstraction for Serverless Computing",excerpt:'For years, cloud computing was a way to convince users and business to spend their money hosting their services on servers collocated on cloud data-centers. Things are changed and merely providing basic storage and processing power is a thing of the past. Cloud services are so complicated today that each one of them needs proper training to just be able to use them. Distributed data-stores, load-balancers, storages, serverless and container automation and orchestration services are just a few example of the vast number of cloud computing services.   One can now have a software running on cloud services (Software-as-a-Service) with five-nine SLA hosted on multiple cloud providers, effectively providing high-availability across the globe, all controlled from a central location, automated and orchestrated with the latest cutting-edge technologies on the market. And the number of these technologies are ever-growing.   Most of the cloud providers today use open-source model to distribute their technologies and software. Examples of which are kubernetes, docker and firecracker. Containerization and light-weight virtual machines are just two of the various ways to deploy applications on cloud. Each has its own pros and cons. But in this article I am going to present firecrack, a light-weight virtual machine, or microVM, management tool created by Amazon to run their serverless platform. In contrast to containerization, specifically docker, which uses a single shared Linux kernel with cgroups, namespaces, etc., microVMs use a separate Linux kernel virtualized on top of kernel-based virtual machine (KVM). The advantages of microVM are less memory overhead (5 MB), very minimal optimized kernel and security. The most interesting part is that it is written in Rust, has an awesome Go SDK and many tools are already developed for it:      https://github.com/firecracker-microvm/firecracker                                                                               firecracker-microvm/firecracker                   Secure and fast microVMs for serverless computing. - firecracker-microvm/firecracker                        https://github.com/              https://github.com/firecracker-microvm/firecracker-containerd                                                                               firecracker-microvm/firecracker-containerd                   firecracker-containerd enables containerd to manage containers as Firecracker microVMs - firecracker-microvm/firecracker-containerd                        https://github.com/              https://github.com/firecracker-microvm/firecracker-go-sdk                                                                               firecracker-microvm/firecracker-go-sdk                   An SDK in Go for the Firecracker microVM API. Contribute to firecracker-microvm/firecracker-go-sdk development by creating an account on GitHub.                        https://github.com/              https://github.com/firecracker-microvm/firectl                                                                               firecracker-microvm/firectl                   firectl is a command-line tool to run Firecracker microVMs - firecracker-microvm/firectl                        https://github.com/              https://github.com/weaveworks/ignite                                                                               weaveworks/ignite                   Ignite a Firecracker microVM. Contribute to weaveworks/ignite development by creating an account on GitHub.                        https://github.com/           There is a demo project demonstrating the abilities and features of microVM by showing how quickly it can run 4k microVMs in less than a minute:      https://github.com/firecracker-microvm/firecracker-demo                                                                               firecracker-microvm/firecracker-demo                   A demo running 4000 Firecracker microVMs. Contribute to firecracker-microvm/firecracker-demo development by creating an account on GitHub.                        https://github.com/           Months ago, I\u2019ve started experimenting with it and wrote a bunch of bash scripts and a simple README to be able to show how to download project binaries or build the project(s) from source and start using microVMs very quickly:      https://github.com/mostafa/firefighter                                                                               mostafa/firefighter                   A set of scripts to download or build firecracker and run a firecracker micro-VM  - mostafa/firefighter                        https://github.com/           In order to use it, just clone the project somewhere on your disk and either run get_latest.sh or build_latest.sh. To get the latest binaries along with pre-built Debian and Alpine kernel and rootfs, you just need to have curl installed. But in order to build it from source code, you should have Rust compiler installed. The easiest way to install Rust compiler and toolchain, just download and install rustup installer to be able to easily install other things. A simple tutorial is present on Rust official website that provides instructions on how to do this. For building the latest version, I assume you have Debian GNU/Linux installed, because of APT.   Although building from source gives you full control and customizability, but for mere experimentation, just download the binaries and related files and you\u2019re good to go!   $ git clone https://github.com/mostafa/firefighter $ cd firefighter $ ./get_latest.sh   This script downloads the latest firecracker, firectl and jailer binaries with a progress bar, along with Alpine and Debian kernel and rootfs in images directory.   Running the following command with no option or with help would print usage instructions:   $ ./run_microvm.sh Usage:   run_microvm.sh start &lt;distro-name&gt; | &lt;vmlinuz.bin&gt; &lt;rootfs.ext4&gt;   run_microvm.sh stop   run_microvm.sh config   run_microvm.sh status   run_microvm.sh help  Available distros:    - debian    - alpine   To run an Alpine microVM, just run the following command, it needs root/sudo privilege:   $ ./run_microvm.sh start alpine   Giving read/write access to KVM to user   [sudo] password for user:   Booting kernel: images/alpine-vmlinuz.bin   Image: images/alpine.ext4   ...   Enable routing from/to MicroVM   Run MicroVM   ...   Welcome to Alpine Linux 3.8   Kernel 4.14.55-84.37.amzn2.x86_64 on an x86_64 (ttyS0)  localhost login:   Using root as username and password, you can login to the microVM. Run the following commands inside the microVM to enable internet access:   $ ip addr add 172.16.0.2/24 dev eth0   $ ip route add default via 172.16.0.1 dev eth0   $ echo "nameserver 8.8.8.8" &gt; /etc/resolv.conf   $ ping -c 3 google.com   PING google.com (172.217.20.46): 56 data bytes   64 bytes from 172.217.20.46: seq=0 ttl=54 time=4.270 ms   64 bytes from 172.217.20.46: seq=1 ttl=54 time=5.979 ms   64 bytes from 172.217.20.46: seq=2 ttl=54 time=6.455 ms  --- google.com ping statistics ---   3 packets transmitted, 3 packets received, 0% packet loss   round-trip min/avg/max = 4.270/5.568/6.455 ms   Now you can stop the microVM with the following command on another terminal:   $ ./run_microvm.sh stop   That was it for now. You should figure out how you can leverage this to your advantage.   It\u2019s a simple open-source project and any feedback and contribution is welcome.  ',categories:["MicroVM","Firecracker","Amazon","Firefighter","Serverless"],tags:["MicroVM","Firecracker","Amazon","Firefighter","Serverless"],url:"https://mostafa.dev/blog/microvm-another-level-of-abstraction-for-serverless-computing",teaser:null},{title:"Load Testing Your API with Swagger/OpenAPI and k6",excerpt:'Throughout the years, there has been many attempts to devise a universal format for defining Web API specifications. The objective was (and still is) to help stakeholders of the system to work with those APIs, without having access to the source code. Each new \u201cuniversal\u201d format came with the promise of being ubiquitous and all-encompassing, but eventually faded away due to various reasons, like OData and WSDL.   One of the major players in this field is Swagger, developed by Tony Tam in 2011, which later was hired by the SmartBear Software, effectively acquiring the rights to the software. In 2015, the same company created a new organization under the sponsorship of the Linux Foundation and called it the OpenAPI Initiative. The new initiative has renamed the Swagger specification to OpenAPI Specification (OAS) in 2016. This specification is generally used for developing, interacting and documenting APIs.   There are many use-cases to OpenAPI. The first is to use it as a documentation for your APIs. Usually it is generated from the request handlers along with the schema for your database models and fields. The second is to use it as a means to generate server-side code, that is the actual API. This is particularly useful for those who want to go spec-first, rather than code-first. The third is to use as a means to generate client-side code. With this method, you\u2019ll have stubs generated for all your requests, which can be used to make use of or even test your APIs.   Overall, the OpenAPI specification and the openapi-generator project can help you generate a lot of integrations out of the box from your API specification. Imagine you could have a specification document that can help you generate your server-side code to serve API endpoints, your client-side code for testing and the documentation of your API for developers and testers.   Swagger/OpenAPI load testing   Formerly, the idea of Swagger/OpenAPI load testing was taken less seriously by the developer community. It was due to the fact that only QA people used to work on performance testing. k6 takes load testing to a whole another level by letting developers write their own load test scripts in JavaScript. With the introduction of new tools, like k6, and then the widespread use of Swagger/OpenAPI for API design and documentation, we felt the need that tools for transforming Swagger/OpenAPI specifications to scripts to be used for performance testing should exist.   We thought it would be a good idea to have a tool to generate a load test script out of an OpenAPI specification document. This new generator will then help you easily integrate load testing in your infrastructure. Therefore, we have added a new k6 generator to the openapi-generator project.   By using this tool, as a Swagger/OpenAPI test generator, you can now generate your k6 load test scripts using the same specification documents you used for your APIs. We tried our best to convert almost all the specification to k6 script, but the generated script should still be modified to make it runnable and suitable for your use case.   Once you have a working script, you can easily use it in your Continuous Integration (CI) platform to automate load testing. You can also use our cloud load testing offering, that gives you the ability to run your load tests in a distributed environment from various regions, plus giving you detailed insights on the results of your test(s).   API load testing with Swagger/OpenAPI specification   We have written a guide for API load testing and since OpenAPI is concerned with APIs, we advise you to take a look at the guide. It will help you understand why you should load test your APIs. The guide gives you an overview of different approaches to API load testing. It includes various ways to create load test scripts, from writing one yourself, to using your existing Postman collections or HAR files and converting them to load test scripts. It also provides you with considerations you need to know while load testing your APIs.   Generating load test script from Swagger/OpenAPI specification   There are various ways to install the openapi-generator. It is distributed in source code, so that you can build it yourself. You have lots of options to use binaries for your operating system. We recommend using the Docker image, which is going to be explained in this article.   I assume you have docker installed, otherwise have a look at the installation instructions for your operating system. The following commands pulls the Docker image for you. You can also omit this command in favor of the next one, because it pulls the latest image for you, if you don\u2019t have it on your machine.   $ docker pull openapitools/openapi-generator-cli   The next step is to run the command inside a container to convert the OpenAPI specification file for you. Assuming you are running Docker on Linux-based operating systems, the following command mounts your present working directory (e.g. /home/user/Desktop) to /local on the container. Also note that after conversion, the container is removed. The container runs the command inside it with the generate option, along with its parameters.   $ docker run --rm -v ${PWD}:/local openapitools/openapi-generator-cli generate \\     -i http://httpbin.test.loadimpact.com/spec.json \\     -g k6 \\     -o /local/k6-test/     --skip-validate-spec   With the above command, we\u2019ve converted the API specification document of the httpbin.org instance, that is hosted on our servers. The parameters are:      The -i parameter is used to feed the specification document to the generator. You can use a file for that purpose. This way you should make sure that it resides in the same directory on your host (/home/user/Desktop/myspec.json), and you address it from the container side: /local/myspec.json. You also have the option to specify a URL that points to a file. Note that the file can be in either JSON or YAML format.   The -g parameter specifies the generator to use, that is k6.   The -o parameter specifies the directory to store the generated files. After conversion, the files can be accessed from /home/user/Desktop/k6-test/ on your host.   In case you get a lot of errors from validation, just skip them with --skip-validate-spec. These errors are often related to differences between Swagger and OpenAPI specification.   Load testing using the generated script   The script should definitely be cleaned up after conversion. The original script is too long to fit into this article, thus the following example is an snippet of the generated script. Extra comments and unneeded code has been removed for demonstration purposes and the n variable is initialized.   import http from "k6/http"; import { group, check, sleep } from "k6";  const BASE_URL = "https://httpbin.org/"; const SLEEP_DURATION = 0.1;  export default function() {   group("/absolute-redirect/{n}", () =&gt; {     let n = 2;     let url = BASE_URL + `/absolute-redirect/${n}`;     // Request No. 1     let request = http.get(url);     sleep(SLEEP_DURATION);   }); }   Running the above script has produced the following output:                     k6 output            Considerations for the generated script   Take these into consideration while converting your OpenAPI specification documents to k6 scripts:      The order of the requests follow the order in the specification document and may not always be correct. Replace them to match your request/response flow. For example, you first create a resource with POST, then you read it with GET.   All requests belonging to the same path are grouped together with the k6 group feature.   A global sleep duration is defined with the SLEEP_DURATION variable and for each request, the value is applied using the k6 sleep function. Change them to match the delay you need between the requests.   Make sure the BASE_URL is correct.   Initialize all variables that their value starts with "TODO_EDIT_THE_...".   Body parameters are in the form of { variable: "datatype" }. Change the datatype to the desired value. Nested body parameters follow the same pattern.   For file upload endpoints that use multipart requests, replace the file name with your own.   You may encounter undefined variables being used. Just add them manually. It may be due to differences between OpenAPI and Swagger specifications.   We only use checks for the default and 200 OK responses for now. It is up to you to check for the rest.   If you found any issues, please file it on issues.   Conclusion   The k6 generator for OpenAPI was written to onboard users to k6. The tool is going to help users quickly generate a load test script out of their existing Swagger/OpenAPI specification documents. This auto-generation of the load test script will help streamline the API testing process, keeping on par with the latest changes to their APIs and specifications.  ',categories:["k6","Open API","Swagger","Load Testing","Performance Testing"],tags:["k6","Open API","Swagger","Load Testing","Performance Testing"],url:"https://mostafa.dev/blog/load-testing-your-api-with-swagger-openapi-and-k6",teaser:null},{title:"Load Testing Your API with Postman",excerpt:'In this article, I\u2019ll explain how to use a Postman collection I have created to load test our instance of our test API. The process is pretty straightforward, as is shown below. You need to feed your exported Postman collection to our postman-to-k6 converter, and use the generated k6 script to load test your own API.   # convert postman collection to k6 test postman-to-k6 test-api.json -e env.json -o k6-script.js  # run load test k6 run --vus 100 --duration 5m k6-script.js   Our Test API &amp; Its Testing Scenario   In order to demonstrate the power of k6 in different scenarios, we have created our test API with various example endpoints, which is available at test-api.k6.io. These endpoints are available in the Postman collection:   Public APIs      List all public crocodiles   Get a single public crocodile   Registration and authentication      Register a new user   Bearer/JWT token authentication   Private APIs      List all your crocodiles   Get a single crocodile   Create a new crocodile (max 100)   Update your crocodile   Update selected fields on your crocodile   Remove your crocodile   The scenario is to test all the public and private APIs. For the private APIs, a user is created and its token is extracted. The extracted token is used to make other API calls. The order is very important in the private APIs, since you cannot delete a non-existing resource, for example. By the way, crocodile is our mascot.   Our Test API Postman Collection   To ease testing of our test API and demonstrate the usage of our Postman to k6 converter, I\u2019ve created a Postman collection with almost all of our test API requests. You\u2019ll see how you can access this Postman collection shortly.      This collection includes a set of collection variables, environment variables, pre-scripts, tests, authorization with two different mechanisms, and usages of the Postman Sandbox API.   Load Testing Our Test API with The Postman Collection   We have created a tool that converts your Postman collection to k6 script, which is called postman-to-k6. You can read more about its features in its release notes.   In order to convert your Postman collection to k6 script, you should take the following steps:   1. Optional: Clone the repository and skip to the step 5:   I\u2019ve created a repository for this article that contains the exported Postman collection, along with the converted script and related files. You can clone the repository and import the test-api.json and env.json files into the Postman application and possibly play with them if you want.   This repository contains everything that is needed for load testing our test API, so you can skip to step 4. When using your own collection, you should take all the steps to be able to have a k6 script out of your Postman collection, and to be able to run your load test with it.   $ git clone https://github.com/k6io/example-postman-collection.git   2. Install Node.js (if you haven\u2019t already done so):   For this, I highly recommend that you use something like nvm, which is a Node.js version manager that you can use to have multiple version of Node.js at the same time on your machine and be able to switch to any of them quickly.   3. Install the postman-to-k6 tool:   The postman-to-k6 tool is developed to help you convert the requests inside your Postman collections to k6 scripts, which are actually JavaScript code.   $ npm install -g postman-to-k6   4. Convert your exported Postman collection to k6 script:   Assuming your exported collection is named test-api.json, you can run this command to convert it to a k6 script. The env.json includes all your environment variables that are exported from Postman.   $ postman-to-k6 test-api.json -e env.json -o k6-script.js   If you need more fine-tuning of your test (like we did above), like adding data or changing environment variables inside your code, just take a look at the Options section of the postman-to-k6 README.   The script generated by the converter should look like below. As you see, I\u2019ve manually added the duration (of the test run) to be 1 minute and also added the virtual users (VU) count. These two options let the script run for a minute with 100 virtual users. These 100 VUs try to make as many requests as they can to test the server, which you\u2019ll see in the next screenshot.   import "./libs/shim/core.js"; import "./libs/shim/urijs.js"; import URI from "./libs/urijs.js"; import {   group } from "k6";  export let options = {   maxRedirects: 4,   duration: "1m",   vus: 100 };  const Request = Symbol.for("request"); postman[Symbol.for("initial")]({     options,     collection: {         BASE_URL: "https://test-api.k6.io/"     },     environment: {         USERNAME: "test@example.com",         PASSWORD: "superCroc2020",         FIRSTNAME: "John",         LASTNAME: "Doe",         EMAIL: "test@example.com",         ACCESS: null,         REFRESH: null,         CROCID: null     } });  export default function () {     group("Public APIs", function () {         postman[Request]({             name: "List all public crocodiles",             id: "3ddd46c4-1618-4883-82ff-1b1e3a5f1091",             method: "GET",             address: "/public/crocodiles/"         });          postman[Request]({             name: "Get a single public crocodile",             id: "9625f17a-b739-4f91-af99-fba1d898953b",             method: "GET",             address: "/public/crocodiles/1/"         });     });      // NOTE: The rest of the requests can be accessed     // from the repository in step 1 });   The generated script is a little bit different from normal k6 scripts, since it includes various abstractions to support different Postman functionality, but you can mix them with regular http requests from k6. Also, there is a libs directory beside the script that includes shims and libraries needed for the Postman scripts to work correctly.   5. Install k6:   k6 supports various platforms, including Windows, Linux, macOS and docker. In order to install it, just grab an Windows installer or a docker image and run it on your machine. On Linux distributions, you can use APT or YUM, and on macOS, you can use Homebrew.   NOTE: Regarding installation on Windows, you can also use choco k6 package.   6. Run k6 with the generated script:   Now that you have your collections converted into a k6 script, you can invoke k6 like this:   $ k6 run k6-script.js   The result of running the script is shown in the following console output:              /\\      |\u203e\u203e|  /\u203e\u203e/  /\u203e/         /\\  /  \\     |  |_/  /  / /         /  \\/    \\    |      |  /  \u203e\u203e\\      /          \\   |  |\u203e\\  \\ | (_) |    / __________ \\  |__|  \\__\\ \\___/ .io    execution: local      output: -      script: k6-script.js      duration: 1m0s, iterations: -          vus: 100,  max: 100      done [==========================================================] 1m0s / 1m0s      \u2588 Public APIs      \u2588 Registration and authentication      \u2588 Private APIs      data_received..............: 8.8 MB 146 kB/s     data_sent..................: 4.8 MB 80 kB/s     group_duration.............: avg=753.07ms min=239.15ms med=495ms    max=4.06s    p(90)=1.37s    p(95)=1.73s     http_req_blocked...........: avg=12.31ms  min=362ns    med=1.52\xb5s   max=3.47s    p(90)=1.83\xb5s   p(95)=1.96\xb5s     http_req_connecting........: avg=1.95ms   min=0s       med=0s       max=779.59ms p(90)=0s       p(95)=0s     http_req_duration..........: avg=211.11ms min=104.42ms med=183.12ms max=924.43ms p(90)=304.25ms p(95)=404.24ms     http_req_receiving.........: avg=1ms      min=41.14\xb5s  med=169.38\xb5s max=130.94ms p(90)=328.31\xb5s p(95)=2.22ms     http_req_sending...........: avg=205.91\xb5s min=38.06\xb5s  med=163.76\xb5s max=113.06ms p(90)=258.45\xb5s p(95)=302.86\xb5s     http_req_tls_handshaking...: avg=8.69ms   min=0s       med=0s       max=2.43s    p(90)=0s       p(95)=0s     http_req_waiting...........: avg=209.9ms  min=104.05ms med=182.22ms max=891.77ms p(90)=301.29ms p(95)=402.41ms     http_reqs..................: 26363  439.382653/s     iteration_duration.........: avg=2.28s    min=1.43s    med=2.01s    max=6.55s    p(90)=2.86s    p(95)=3.64s     iterations.................: 2588   43.133267/s     vus........................: 100    min=100 max=100     vus_max....................: 100    min=100 max=100    Remarks about using the postman-to-k6 converter   1\ufe0f. Should we base our load tests on the Postman converter and our Postman collections?   If you\u2019re using the converter as a way of onboarding, no. If you expect to convert your collection continuously and without doing a lot of manual edits afterwards, yes.   We recommend you to use the converter as an easy way to onboard and then rewrite your scripts to idiomatic k6 code, as we believe it to be more maintainable and less likely to degrade over time. If you convert from postman collections continuously, however, and run the script output as-is, it might make sense to keep it as is.   2. Is everything available out of the box in the converted script?   No. Since k6 uses Goja to run JavaScript, and it is not compatible with browsers\u2019 and Node.js APIs, hence there are some missing functionality. This can be fixed by importing bundled JavaScript modules. For a list of compatible libraries, please see jslib.k6.io.   3. What adjustments did you make to the script to make it work?   First, I removed the pre-script containing pm.sendRequest, because it is not supported by the converter. Then, I replaced the jsonData.hasOwnProperty syntax with the equivalent k6 syntax for extracting JSON response information: response.json("selector").      Postman API vs. k6 API   Here\u2019s a quick comparison of the Postman API versus the k6 API. To be fair, I have included features from Postman GUI application. Since k6 is scriptable from the start, you have the option to write the logic in JavaScript. Postman also supports javascript to do various tasks, but the focus is on exposing features via a richer set of GUI elements.                  Feature       Postman API       k6 API                       Importing external libraries       Selected libraries       Selected libraries plus bundled ones (non-browser, non-Node.js APIs)                 Making requests       \u2705       \u2705                 Processing response       \u2705       \u2705                 Parametrization       \u2705       \u2705                 REST       \u2705       \u2705                 GraphQL       \u2705       \u2705                 Cookies       \u2705       \u2705                 Proxy       \u2705       \u2705                 SSL       \u2705       \u2705                 OpenAPI/Swagger       \u2705 (import directly)       \u2705 (via k6 generator in openapi-generator)                 Checks       \u2705 (assertions)       \u2705 (Check API)                 Groups       \u2705 (Collections)       \u2705 (Group API)                 HTML parsing       \u2705 (needs library)       \u2705 (internal HTML API)                 File upload       \u2705       \u2705                 Test Lifecycle       \u2705 (only with scripts)       \u2705 (internal)           As you saw above, there are many features supported by each API, each to some extent. Some features needs external libraries, some are internal. Both APIs are scriptable in JavaScript, and not everything is supported by both, due to the various browser and Node.js APIs used in the libraries.   Yet, there are some features only available on k6, which is partially due to the fact that the Postman is catered for API testing or API functional testing, but k6 is focused more on API load testing.   Functional testing vs. load testing   Functional testing concerns with giving input to the system (as a black-box) via an API and examining the results, while load testing is basically doing the same thing as functional testing, but with additional load on the input to the system.   Functional testing provides input on each endpoint, and the returned results are verified in terms of correctness against a set of specifications. In turn, load testing provides a huge amount of load on each endpoint, and rather tries to aggregate the metadata returned by all the responses.   Load testing metrics for measuring performance   Concerning the measurements, the metadata will include the time it took for the request to settle and the response to return, which are measure by various metrics. For example you can measure the HTTP request duration of all requests and get their minimum, maximum, average, median, 90th and 95th percentiles.   Pass/fail a test with thresholds   You also have the option to pass/fail a test if it does/doesn\u2019t reach certain threshold(s). For example, you can specify that you want the average response time to be less than 500ms. If the average is below that, the test will fail, much like asserts in software testing.   Filter results with tags   Since you\u2019re dealing with lots of different results from different endpoints, your life would be easier if you could filter the results. Tags are supported by k6 to fulfill this requirement.   Load testing WebSocket servers   In terms of protocol implementation, WebSocket is one of the features available only in k6, compared to Postman, and you can load test your WebSocket server with it.   Conclusion   In this article I\u2019ve tried to give a quick introduction to Postman, the postman-to-k6 converter and our k6 load testing tool. All these tools combined can help you turn your API requests in Postman into k6 script in order to load test your API. Many of the Postman features are supported by the postman-to-k6 tool.   Our ultimate goal is to streamline the process of onboarding you to our load testing tool, k6. In doing so, we have created a bunch of integrations that can help you start load testing in your infrastructure.  ',categories:["k6","Postman","Load Testing","Converter","JavaScript"],tags:["k6","Postman","Load Testing","Converter","JavaScript"],url:"https://mostafa.dev/blog/load-testing-your-api-with-postman",teaser:null},{title:"DevRel Content Strategies",
excerpt:"The very first developer relations online meetup of Stockholm was streamed live on Facebook on April 15, 2020. It was also recorded and later published on YouTube for those who couldn\u2019t attend.   The event was organized and streamed by Oleg Pridiuksson, who is a decade-old experienced developer advocate who worked for great companies like King and Unity. He is now a developer relations program consultant and basically founded the Developer Relations Sweden meetup group.   There were three speakers: Hassan Al Kazmi, who is an artist advocate at King, who talked and answered questions about \u201chow much support should a developer advocate provide\u201d. The next speaker was Janina \u0141aszkiewicz, an agile coach, community manager, founder of Eventspace.by, who also talked and answered questions about \u201cdoing events so developers like them 3000\u201d. Their talks were more of an unconference, where Oleg used to ask questions and the speakers would answer and discuss the answers. You can watch the 5 minutes summary of the online event.            I went into reverse by presenting with slides about \u201cDevRel content strategies with a focus on SEO optimization for developers\u201d, instead of the casual unconference style.            My talk was based on an article, named \u201cRisk vs. Reward: How to Build a Diversified Content Portfolio\u201d by Ryan Law. I took this article as the basis of my talk and then added the sauce of experience on my Medium articles about one single article about security, \u201cSecure Code Review and Penetration Testing of Node.js and JavaScript Apps\u201d, and a series of articles I\u2019ve published about load testing with k6. I took the stats from Medium and tried to re-frame and present them in a way the conforms to the recommendations laid out in the article.   Although I have presented the talk on a DevRel event and it is named \u201cDevRel Content Strategies\u201d, it is still beneficial to anyone publishing content over the internet, no matter the area of interest.   I hope you\u2019ll find it useful and I am happy to hear your feedback.  ",categories:["SEO","Content Marketing","Developer Relations","Content Strategy","Blogging"],tags:["SEO","Content Marketing","Developer Relations","Content Strategy","Blogging"],url:"https://mostafa.dev/blog/devrel-content-strategies",teaser:null}];